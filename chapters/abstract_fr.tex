\chapter*{Résumé}

La complexité grandissante des systèmes dynamiques rend leur modélisation et leur contrôle de plus en plus difficiles.\emph{ Apprendre} à contrôler ces systèmes directement à partir de données est donc une alternative intéressante. Pour utiliser de tels contrôleurs sur des systèmes physiques, il est important de s'assurer qu'ils se comportent de façon sûre. Ces garanties sont souvent difficiles à obtenir, et nécessitent de modéliser partiellement le système: cela diminue fortement l'intérêt de méthodes sans modèle telles que l'apprentissage par renforcement. Trouver comment apprendre un comportement sûr sans modèle est donc un défi majeur pour appliquer ce dernier à des systèmes physiques. Dans cette thèse, nous étudions la relation entre deux approches communément opposées: pénaliser un ensemble d'états d'échec, ou contraindre l'agent à ne pas explorer cet ensemble.\par
La première contribution majeure de ce travail est de prouver que, tant que la pénalité est assez grande, pénaliser l'échec fournit les mêmes garanties de sécurité que de contraindre l'agent à ne pas échouer. Les deux approches sont donc équivalentes, et trouver une valeur pour la pénalité ne nécessite qu'une connaissance minime du système.\par
Notre seconde contribution est l'utilisation de cette équivalence théorique dans des algorithmes apprenant un comportement sûr tout en minimisant le nombre d'échecs durant la période d'entraînement.\par
Ces deux contributions utilisent la théorie de la viabilité pour fournir une nouvelle interprétation théorique du problème d'apprentissage d'un comportement sûr. Nous démontrons que ce problème consiste simplement à contraindre l'agent à rester dans le plus grand ensemble d'états sûrs: le noyau de viabilité. Ainsi, un agent sûr n'explore qu'un certain sous-ensemble de l'ensemble des états-actions, ce qui est peu commun. La seconde conséquence est que le noyau de viabilité permet de définir la sécurité d'un agent sans modéliser le système sous-jacent. Il s'agit donc d'un outil très prometteur pour définir et apprendre la sécurité sans modèle.\par
Cette thèse contient également une troisième contribution portant sur l'influence de la paramétrisation sur l'apprentissage de politiques sûres par pénalisation. Nous développons un contre-exemple montrant que les politiques paramétrées apprises par pénalisation ne bénéficient d'aucune garantie de sécurité en général. De plus, le saut de dualité entre les approches par pénalisation et par contrainte ne permet pas d'inférer le risque de la politique optimale apprise par pénalisation. Ce résultat affaiblit les conclusions de travaux récents~\cite{paternain2019safe}, où le saut de dualité est minimisé afin de résoudre approximativement le problème sous contraintes.