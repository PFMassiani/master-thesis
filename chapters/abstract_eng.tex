\chapter*{Abstract}

% Motivation: learning from data is now popular (RL)
As the complexity of systems keeps increasing, learning controllers directly from data becomes an appealing alternative to model-based controller design. Ensuring that these controllers behave safely is critical in order to use them on real-world systems. Yet, giving such guarantees is generally hard, and requires a lot of system knowledge: this largely diminishes the interest of model-free approaches such as reinforcement learning (RL). Finding a general, model-free way to provably learn safe behaviours is a key challenge for RL. This thesis examines the relationship between two approaches to this question: penalizing a set of failure states, and constraining the agent to avoid these states.\par
The first major contribution of this thesis is proving that, as long as the penalty is large enough, penalizing failures gives the same safety guarantees as constraining the agent. Therefore, the two problems are theoretically equivalent and the penalty can be provably scaled with minimal system knowledge.\par
The second contribution is to demonstrate empirically how this theoretical equivalence can be used to learn safe policies while reducing the number of failures during training. \par
These two contributions are based on a new theoretical understanding of the safe learning problem enabled by viability theory. We show that learning a policy that avoids a set of failure states boils down to constraining the agent to stay in the largest possible safe set: the viability kernel. The first consequence is that the safe learning problem can be treated as a simple, unconstrained problem in a subset of the state-action space, which is an uncommon property. The second consequence is that the viability kernel provides a general, model-free answer to the question of what staying safe means.\par
This thesis also includes a third contribution on the impact of parameterization on learning safe policies through penalization. We provide a counterexample showing that the parameterized, optimal policy learned by penalizing failures does not enjoy any safety guarantees in general. We demonstrate that the duality gap between the penalized and the constrained problems is a very poor indicator of the risk. This result weakens the conclusions of recent work~\cite{paternain2019safe} where this duality gap is minimized to approximately solve the constrained problem.