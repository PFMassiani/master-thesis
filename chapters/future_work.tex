\chapter{Summary and outlook}

\section{Summary} \label{sec:summary}
This thesis tackles the problem of quickly learning reward-optimal safe policies. Learning safe policies in a sample efficient way is of critical importance when implementing learning methods on dynamical systems and real robots. Theoretically-guaranteed methods exist to solve learning problems with safety constraints, but are generally model-based. The question this thesis focuses on is therefore whether it is possible to\emph{ learn} these constraints while learning control at the same time, in a purely model-free setting.\par
The first contribution is a theoretical characterization of safe policies using tools from the well-established viability theory. This enables us to demonstrate a peculiar property of learning with a $0$-risk constraint compared to classical CMDPs: it  behaves exactly as an unconstrained problem on a subset of the state-action space; the viable set. From this property, we derive a new result on penalty scaling showing that penalizing failures is a theoretically guaranteed way of learning safe optimal policies, and this for an unbounded interval of penalties. This provides an answer to recurring concerns about penalty-based methods. However, we also demonstrate on a counterexample that these guarantees do not hold when policies are parameterized - which is almost always the case in practice. We argue that, in the case of parameterized policies, penalizing failures to have a return similar to the one of the constrained problem does not provide\emph{ any} guarantees on the safety of the learned policy. Hence, penalty-based approaches do not enable to approximately solve the safe RL problem when theoretical guarantees do not hold.\par
Therefore, we build up on previous work estimating the viable set of a system and propose three variants of an algorithm to learn safe policies. These algorithms construct an estimate of the viable set, and restrict their exploration policy in this estimate to ensure approximately safe exploration. We benchmark these algorithms with penalized Q-Learning on two environments: a spaceship subject to gravity, and the spring-loaded inverted pendulum model. These systems exhibit different types of challenges: the first one has a large unviable set, and therefore requires long-term planning to avoid failure, whereas the second one has complex dynamics and can easily fail. This benchmark experimentally demonstrates that the viability-based approach of safety can learn optimal safe policies in a sample-efficient way, and offers and interesting comparison between penalty-based and constraints-based methods.\par
This thesis bridges the gap between the chance-constrained formulation of safe reinforcement learning and penalized methods. It also provides insight on what the effect of constraining Markov decision processes is, and takes a first step in using this insight to solve safety-constrained MDPs. As the interest for learning control for dynamical systems grows, we believe that this works contributes to unifying the different approaches to safety in model-free optimal control.

\section{Outlook}

\subsection{Allowing a nonzero risk of failing} \label{sec:future nonzero}
By defining safety as allowing a $0$ risk of ending up in the failure set, we are able to treat the resulting CMDP as a simple MDP with a modified state-action space: the viable set \QV. In practice, specialized algorithms are still required in order to\emph{ learn} the constraint, but this theoretical decoupling between the unviable set and the viable set allows us to use a lot of tools from classical MDP theory.\par
We believe that using the viable set to characterize safe policies is a very promising perspective, and can be extended to the more general setting of allowing a non-zero risk $\varepsilon$ of ending up in the failure set. With this definition, safe policies should be the ones that leave the viable set with probability at most $\varepsilon$. From this simple viability theory result, safe policies could be easily characterized as the ones that only put so much probability mass they on actions in the critical set \QC, similarly to what Corollary~\ref{clry:safe policies on qc} does.
% 0 risk setting is helpful, but generalizing would be interesting for safe learning algorithms
% policies are randomized
\subsection{Parameterization and strong duality}
We have given in Section~\ref{sec:parameterization} counterexamples where parameterizing the set of policies breaks the safety guarantees that the penalized problem enjoys. Deriving necessary conditions on the parameterization for strong duality to still hold could be very beneficial. Such conditions could be for example based on classic results from duality theory (Karush-Kuhn-Tucker conditions~\cite{boyd2004convex}, ...) .\par
This differs from the work of~\textcite{paternain2019safe}, on which we have already commented in Section~\ref{sec:parameterization}: the authors study the difference between the parameterized penalized problem and the unparameterized constrained one. What we suggest here is to study when penalizing policies can be done without sacrificing performance compared to the parameterized constrained problem (which is in any case the best solution that can be found).

\subsection{Scaling the algorithm}
The three novel algorithms presented in Chapter~\ref{chap:benchmark} scale very poorly with the system's dimensionality. The main reason for that is the use of Gaussian processes as function approximators. Indeed, estimating the safety measure in higher dimensions requires a lot of samples, and GPs scale very poorly with the size of their dataset. Therefore, replacing them with other function approximators (like neural networks) is a very promising approach. This could be done for example by having a first network predict the set of viable actions in a given state. This output could then be fed into a policy network, whose role is to pick the best action. Such an architecture would extend the idea that we have explored here of constraining the exploration policy into an estimated safe set.