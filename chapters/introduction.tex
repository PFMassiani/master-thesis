\chapter{Introduction} \label{chap:intro}

\section{Motivation}
	\emph{How can robots learn how to act safely model-free ?} \par
	In the past 30 years, \emph{reinforcement learning} (RL) has found its way into the control engineer's toolbox. Its main promise is the ability to learn controllers without having to model the\emph{ dynamical system} at hand. Such models are indeed critical for conventional control approaches, but deriving them from first principles is often hard and requires a lot of domain knowledge. Moreover, the guarantees derived this way are only as good as the underlying models, and controllers that are optimal for the model may perform quite poorly on real-world robots~\needcite. With the ever-increasing availability of experimental data and computational power, learning optimal controllers directly from that data has quickly become an appealing alternative. While RL has historically been focused on solving tasks in controlled environments like video games~\needcite, a lot of research effort has recently been put in applying it to real-world systems~\needcite, where dynamics are often hard to model. As a matter of fact, RL shares a lot of common traits with optimal control~\cite{sutton1992reinforcement}, with the major difference that the former is model-free and data-driven. \par
	One key advantage traditional approaches still have is the additional\emph{ safety} guarantees they come with. Indeed, there is generally no way of knowing\emph{ a priori} what\emph{ policy} a RL agent learns, and in practice, such policies can behave spectacularly differently from what is expected. While this is usually seen as a feature in optimization-based controller design~\cite{baker2019emergent}, it can quickly become problematic in the traditional RL framework where the engineer's only degree of freedom\footnote{Once the problem is modeled.} is the\emph{ reward function}. Designing such a function then becomes a very complex task, since it needs to reflect all safety concerns (so learned controllers are guaranteed safe) without interfering with the function's primary role: specifying the task the agent should learn. There has been some recent interest in guaranteeing the safety of learned controllers when such a function is available~\cite{hans2008safe}~\cite{turchetta2016safe}~\cite{berkenkamp2016safe}, but designing it typically requires a lot of system knowledge~\needcite. This exhibits the paradox of safe RL, where system knowledge is used to craft complex reward functions that are then used to teach a model-free algorithm.\par
	The idea of putting together safety concerns and task specification in the same reward function can also be criticized. Indeed, it seems inappropriate to put on the same scale actions optimizing a process and constraints that should never violated, no matter how much reward this produces. Think of controlling a nuclear power plant for example, where core meltdown should be avoided at all costs: simply penalizing such a state does just not seem right.\par
	A key question that stems from these two considerations is then: is it possible to design agents that can\emph{ provably} learn safety constraints from a simple reward signal?
%	It is well-known among roboticists that naively training RL agents on simulations and transferring the resulting controllers on hardware generally fails. A lot of research effort has been recentely directed at bridging this\emph{ sim-to-real} gap; either by deriving principled and efficient ways of using real-world data in the training process~\cite{hwangbo2019learning}~\cite{marcobayesian}. \TODO{Rephrase this paragraph} The policy learned that way can indeed only be as good as the simulator, and unmodeled dynamics or perturbations can severely damage the robot or the world. Hence, training on hardware is a required step in applying RL to robotics. This makes the concerns about safe learning even more critical: we not only need to ensure that the learned policy does not take unsafe actions (unlike in CoastRunner), but also that these actions are avoided \emph{during the learning process}.
	
\section{Related work}
	There are many very different approaches to safety in RL. In this thesis, we define it as avoiding at all times a set of failures, possibly with some non-zero probability\footnote{Only in this section, where we discuss related work: our own results only apply when safety means having a zero probability of failing.}. We briefly review here the most common approaches to learning safe controllers with RL.
	
	\subsection{Constrained Markov decision processes} \label{subsec:CMDPs related work}
		The classical formulation of RL is based on Markov decision processes (MDPs). Therefore, Constrained Markov decision processes (CMDPs) are the most natural formalization~\cite{garcia2015comprehensive} of safety in RL. The core idea is to introduce other metrics of how well the agent is doing, and to constrain them. Mathematically speaking, a CMDP is simply a constrained optimization problem over the set of policies and with the reward function as an objective. The two major questions they raise are: what is a good constraint? How do we solve a CMDP?
		
		\paragraph{Ergodicity-based safety} When exploring an environment, an agent may take irreversible actions that impact the set it is able to reach in the future. Such a policy in such an environment is called\emph{ nonergodic}. In~\cite{moldovan2012safe}, Moldovan and Abbeel define safety as only following ergodic policies, and formulate and solve a CMDP ensuring that chosen policy is ergodic with at least some probability. This effectively constraints the agent to stay in the\emph{ ergodic set} of its initial state. In~\cite{turchetta2016safe}, the authors assume that this set can be described by a Lipschitz safety function and derive a scalable model-based algorithm capable of exploring the whole ergodic set without ever leaving it. While such a definition of safety can be sufficient for a class of applications, it often only yields very conservative policies. Indeed, the ergodic set is in general only a subset of the set of safe states, and an agent may be perfectly fine after taking a non-ergodic action. Another limitation of this definition is that the ergodic set may itself contain undesired states: consider for example a quadcopter bumping against a wall. The quadcopter may be fine after the shock, but we still want to avoid such a situation.
		
		\paragraph{Chance constraints} A generalization of this ergodic constraint that can be used to define CMDPs is called\emph{ chance constraints}. The user defines a\emph{ safe set} (resp.\emph{ failure set}), and the agent is constrained to stay inside (resp. outside) that set up to some acceptable\emph{ risk}. The main perk of this definition is that it captures very well the definition that we gave of safety, provided that the safe or failure set is defined correctly. The main challenge they pose is that the underlying CMDPs are generally hard to solve. It is also tricky to ensure that such problems are even feasible: it may not be possible to ensure failure from any initial condition, or the considered safe set may not be controllable invariant\footnote{This means that no policy can ensure that the agent stays in it.}. Designing safe sets or acceptable sets of initial states generally requires a lot of system knowledge, and is therefore challenging in the model-free setting. This approach has been explored in~\cite{paternain2019safe} or~\cite{geibel2005risk}, and feasibility is generally assumed. 
		
		\paragraph{Cost constraints} The final type of constraints that we mention here are\emph{ cost constraints}. These are the most general type of constraints, since the two previous cases can generally be reformulated in terms of costs. In this setting, the agent collects one or many\emph{ costs} at each timestep. It then incurs these costs in a total cost function, whose expected value should be bounded\footnote{Constraining other moments of the cost is possible, but the expected value is the most common.}. This problem has been extensively studied~\cite{garcia2015comprehensive}~\cite{kim2012cost}, both in the model-based and model-free settings, but the biggest milestone it probably the Constrained Policy Optimization~\cite{achiam2017constrained} algorithm (CPO) by Achiam et al., which is the first scalable algorithm for cost-constrained MDPs suitable for deep RL. To this day, it is however unclear how such algorithms can be extended to methods other than policy gradients~\cite{chow2018lyapunov}. \par
		There is no general answer to whether cost constraints give a satisfactory definition of safety. As mentioned, some costs may be interpreted in the setting of chance constraints, but not all costs enjoy such a property. For instance, constraining the minimal expected return~\cite{hans2008safe} or its variance is highly dependent on the reward function, and is therefore not a suitable definition of safety for all problems~\cite{garcia2015comprehensive}.
		% A generalization of the chance constraints, since they can be expressed as a cost
		% Not all costs are good definitions of safety: variance/return based costs depend on the reward, whereas the whole point is to decouple safety from the reward
		\paragraph{Solving CMDPs} The main piece of litterature in the field of CMDPs is~\cite{altman1999constrained}. In this book, Altman shows the equivalence between a particular type of reward shaping and CMDPs (we will come back to this in Chapter~\ref{chap:prelim}), and uses it to solve CMDPs. To this day, Altman's results are still part of the theoretical foundation of most algorithms solving CMDPs\footnote{The CPO algorithm is actually one of the few that does not use it.}~\cite{zheng2020constrained}. However, most of these approaches are model-based, and often suffer from the curse of dimensionality~\cite{kim2012cost}. Therefore, finding efficient methods to solve CMDPs is still an open problem. Recently, Paternain et al.~\cite{paternain2019safe} have explored an approach based on Lagrangian duality exploiting Altman's results. The results presented in this thesis highlight some of the theoretical and practical shortcomings of this work, and were derived in parallel.
		
	\subsection{Reward shaping} 
	Although CMDPs are the most natural framework to formalize safety, they are not the most practical way of\emph{ enforcing} it. The roboticist's way of communicating its goal to the agent is through the reward function, and making this function safety-aware is called\emph{ reward shaping}.\par
	The simplest and most common way of proceeding is to change the instantaneous reward that the agent collects by penalizing\emph{ failure states}, that is, states that the agent should not explore. \textcite{geibel2005risk} propose an algorithm to solve that problem while empirically tuning the penalty. In particular, they emphasize the close theoretical relation between penalizing failures and constraining the probability of failing. This idea is of critical importance of this thesis, and we formalize their empirical approach in Chapter~\ref{chap:safety from viability}. A recent survey on safe RL~\cite{garcia2015comprehensive} summarizes the three main critics faced by this approach. First, it is generally considered as a heuristics, and policies learned this way generally are not guaranteed to avoid failure states. Second, such policies - when safe - may be overly pessimistic. We will see later that these concerns are, in fact, not justified. More recently,~\textcite{paternain2019safe} have derived a systematic way of scaling that penalty to ensure safety based on Lagrange duality.
\section{Contributions}
	The goal of this thesis is to bridge the gap between the chance-constrained formulation of safe RL and penalty methods. Our major theoretical contribution is showing that penalizing failures is a theoretically guaranteed way of learning an optimal, safe policy, and this for an unbounded interval of penalties. In doing so, we provide a theoretical analysis of our CMDP of interest: the reward maximization problem constrained to policies with a $0$ probability of failing. We demonstrate that this CMDP can be solved with tools for classical MDPs such as value functions by using tools from viability theory~\cite{aubin2011viability}. We then take a first step in using these results by defining and benchmarking algorithms leveraging this new theoretical insight to solve the $0$-risk CMDP. In particular, we compare algorithms that enforce the $0$-risk constraint directly to penalized methods on two environments presenting different challenges, and compare the safety and the performance of the learned policies.
	\paragraph{Outline} The rest of the thesis is structured as followed. In Chapter~\ref{chap:prelim}, we introduce the formalism of MDPs and CMDPs and\emph{ viability theory}, which is critical to formulate our results. Chapter~\ref{chap:safety from viability} presents our main theoretical contributions: characterization of safe policies with viability theory, the strong duality theorem that connects the cost-constrained formulation with the reward shaping one, and the limitations of that theorem. Finally, we describe and benchmark algorithms leveraging these results in Chapter~\ref{chap:benchmark}.