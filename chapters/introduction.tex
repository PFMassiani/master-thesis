\chapter{Introduction} \label{chap:intro}

\section{Motivation}
	\emph{How can robots learn control model-free while acting only safely ?} \par
	In the past 30 years, \emph{reinforcement learning} (RL) has found its way in the control engineer's toolbox. Conventional control approaches rely on accurate models of the \emph{dynamical system} of interest, but deriving them from first principles is hard and requires a lot of domain knowledge. Moreover, the difficulty of obtaining performance guarantees rapidly increases with systems complexity, and such guarantees are only as good as the underlying models. With the ever-increasing availability of computational power and the development of its first model-free algorithms~\cite{sutton1988learning}, RL has quickly become a topic of interest for the control theory community. Its main promise is that the\emph{ agent} - i.e., the robot - learns a complex behaviour from data simply by getting feedback on how well it is performing by the means of a \emph{reward function}, all of this in a model-free setting. This last point makes it very suitable to control complex systems, for which controller synthesis is a demanding and tricky task. In the RL framework, the engineer can - in principle - only care about \emph{what} the agent should do, and leave the \emph{how} to the optimization process.\par
	While RL is shown to be a perfectly valid alternative for the optimal control task~\cite[Section\,3.4]{sutton2018reinforcement}, traditional approaches like robust control or model-predictive control come with additional\emph{ safety} guarantees that standard RL lacks. While probabilistic guarantees on the behaviour of the controller can be given (and, to some extent, designed) in robust control, there is generally no way of knowing\emph{ a priori} what\emph{ policy} a RL agent learns. It is a very common problem in RL that agent \enquote{hacks} the reward function by finding a reward-optimal behaviour that is radically different from the expected one. A famous example was found in the video game CoastRunner~\cite{clark2016faulty}, where the agent learned to maximize the in-game score without completing the track and while consistently crashing in walls and other boats. In this example, the chosen reward function made it interesting for the agent to operate a tradeoff between an immediate, low-reward action (crashing) and the promise of higher rewards in the long run. This sheds light on the general\emph{ safety} challenge in RL: how do we ensure that agents do not take actions that we judge as highly undesirable? \par
	It is well-known among roboticians that training RL agents on simulations only is not good enough to achieve satisfying results on a real robot. The policy learned that way can indeed only be as good as the simulator, and unmodeled dynamics or perturbations can severely damage the robot or the world. Hence, training on hardware is a required step in applying RL to robotics. This makes the concerns about safe learning even more critical: we not only need to ensure that the learned policy does not take unsafe actions (unlike in CoastRunner), but also that these actions are avoided \emph{during the learning process}.
	
\section{Related work}
	So far, we only have given an intuitive definition of what safety in RL is. We have defined a\emph{ safe policy} as one that does not lead the agent to states that are deemed as \enquote{undesirable} or \enquote{dangerous}, or only with some\emph{ risk} considered acceptable. Formalizing this intuition is a modelling task, and is still a hot topic in the RL community as no general consensus has been reached~\cite{garcia2015comprehensive}. We briefly review here a few approaches to formalizing safety in reinforcement learning, and will give our own definition in Chapter~\ref{chap:safety from viability}.
	
	\subsection{Constrained Markov decision processes} \label{subsec:CMDPs related work}
		The classical formulation of RL is based on Markov decision processes (MDPs). Hence, Constrained Markov decision processes (CMDPs) are probably the most natural formalization~\cite{garcia2015comprehensive} of safety in RL. The core idea is to introduce other metrics of how well the agent is doing, and to constrain them. Mathematically speaking, a CMDP is simply a constrained optimization problem over the set of policies and with the reward function as an objective. The only remaining question - and this is where the bone of contention lies - is: what is a good constraint? 
		
		\paragraph{Ergodicity-based safety} When exploring an environment, an agent may find itself unable to go back to its starting point. Such an environment is called \emph{nonergodic}. In~\cite{moldovan2012safe}, Moldovan and Abbeel formulate and solve a CMDP whose constraint is on the probability that the chosen policy is ergodic. Solving this problem ensures that the agent following this policy will come back to its initial state with some user-specified probability. The fundamental assumption behind this definition of safety is that the set of undesired states is the same as the set of states from which the agent cannot come back. While this is suited for a class of applications, this assumption is in general not satisfied: for example, a robot may be perfectly fine after hurting someone. A further restriction of this ergodicity condition can be found in~\cite{turchetta2016safe}. The authors consider a \emph{safety function} describing the failures that are not captured by the ergodicity condition, and derive an algorithm that can - under strong assumptions - explore the ergodic set while respecting a lower threshold on the safety function.
		
		\paragraph{Chance constraints} Another type of constraints that can be used to define CMDPs are\emph{ chance constraints}. The user defines a\emph{ safe set} (resp.\emph{ failure set}), and the agent is constrained to stay inside (resp. outside) that set up to some acceptable\emph{ risk}. The main perk of this definition is that it captures very well the intuitive definition of safety, provided that the safe or failure set is defined correctly. However, choosing such a set and an acceptable risk is highly nontrivial: if the safe set is too small, or the failure set too big, the underlying (unknown) dynamics may make the problem infeasible. This approach has been explored in~\cite{paternain2019safe} or~\cite{geibel2005risk}, and feasibility is generally assumed. 
		
		\paragraph{Cost constraints} The final type of constraints that we mention here are\emph{ cost constraints}. These are the most general type of constraints, since the two previous cases can generally be reformulated in terms of costs. In this setting, the agent collects one or many\emph{ costs} at each timestep. It then incurs these costs in a total cost function, whose expected value should be bounded\footnote{Constraining other moments of the cost is possible, but the expected value is the most common.}. This problem has been extensively studied~\cite{garcia2015comprehensive}~\cite{kim2012cost}, both in the model-based and model-free settings, but the biggest milestone it probably the Constrained Policy Optimization~\cite{achiam2017constrained} algorithm (CPO) by Achiam et al., which is the first scalable algorithm for cost-constrained MDPs suitable for deep RL. To this day, it is however unclear how such algorithms can be extended to methods other than policy gradients~\cite{chow2018lyapunov}. \par
		There is no general answer to whether cost constraints give a satisfactory definition of safety. While some costs may be interpreted in the setting of chance constraints, others lack generality in their definition of risk. For instance, constraining the minimal expected return~\cite{hans2008safe} or its variance is highly dependent on the reward function, and is subject to the same type of criticism~\cite{garcia2015comprehensive} than ergodicity-based safety.
		% A generalization of the chance constraints, since they can be expressed as a cost
		% Not all costs are good definitions of safety: variance/return based costs depend on the reward, whereas the whole point is to decouple safety from the reward
		\paragraph{Solving CMDPs} The main piece of litterature in the field of CMDPs is probably~\cite{altman1999constrained}. In this book, Altman shows the equivalence between a particular type of reward shaping and CMDPs (we will come back to this in Chapter~\ref{chap:prelim}), and uses it to solve CMDPs. To this day, Altman's algorithm is still commonly used\footnote{The CPO algorithm is actually one of the few that does not use it.}~\cite{kim2012cost}~\cite{zheng2020constrained}, but this approach is model-based and suffers from the curse of dimensionality. Hence, finding efficient methods to solve CMDPs is still an open problem. Recently, Paternain et al.~\cite{paternain2019safe} have explored an approach based on Lagrangian duality exploiting Altman's results. Extending these theoretical results and demonstrating their practical shortcomings is actually one of the contributions of this work.
		
	\subsection{Reward shaping} 
	Although CMDPs are the most natural framework to formalize safety, they are not the most practical way of\emph{ enforcing} it. The immediate reflex to do that is to try to cram the safety concerns into the reward function, which is the roboticist's way of telling the agent what to do: this is called\emph{ reward shaping}. \par	
	There are many ways that the reward function can be modified to account for safety concerns. The historic ones consist in penalizing policies with high variance, or more generally, in optimizing for an exponential utility instead of the total expected reward. The main drawback of these approaches is that formulating RL algorithms to solve them is difficult, as optimal policies no longer satisfy a Bellman equation~\cite{mihatsch2002risk}. Another approach is to optimize not the expected reward but a worst-case criterion, but the resulting policies are usually overly pessimistic. For a more detailed discussion of these approaches, see~\cite{garcia2015comprehensive}. Another technique for reward shaping is to change the instantaneous reward that the agent collects, thus\emph{ penalizing} some states that the agent should not explore. Geibel and Wysotzki~\cite{geibel2005risk} propose an algorithm to solve that problem while scaling the penalty level. More recently, Paternain et al.~\cite{paternain2019safe} derive a systematic way of scaling that penalty to ensure safety.\par
	These techniques have the obvious advantage of being very easy to implement. However, they are largely considered as heuristics. Even though agents trained through reward shaping can act safetly, there is no guarantee that they will (like in CoastRunner~\cite{clark2016faulty}), and when they do, it is unkown whether their behaviour is optimal. As a matter of fact, setting too high a penalty has been shown to largely impair the learning process~\cite{ray2019benchmarking}. It is also unclear what states should be penalized: is penalizing failure states only enough, or should we also penalize the\emph{ unviable} states that lead unconditionally to failure? Another important question that remains to be answered is whether these penalty methods can be used to reduce the number of failures during training.
	% More a way of solving the problem than a way of defining safety
	% Barrier functions, variance based penalizations, set penalizations

\section{Contributions}
	In this work, we focus on the problem of ensuring safety in a model-free RL setting. Since completely avoiding failure without any further assumptions is obviously an impossible task, we will consider a setting where failure is costly, but not catastrophic: the agent is allowed to fail in the learning process, but we want to quickly reduce the number of failures. Our goal is to derive a\emph{ practical} algorithm that can be implemented on hardware, while keeping as much as the theoretical guarantees as possible. Hence our contributions are the following:
	\begin{itemize}
		\item Propose a novel definition of safety that can be equivalently stated in terms of chance constraints, cost constraints, or reward shaping, thus being at the same time easily interpretable and computationally tractable;
		\item Derive an easy to implement and theoretically-guaranteed way of scaling penalties, extending the work of~\cite{altman1999constrained} and~\cite{paternain2019safe};
		\item Give insight on the practical limitations of the reward-shaping technique, providing intuition on when encoding safety through penalties can yield unsafe policies;
		\item Develop and benchmark an algorithm to directly solve chance-constrained MDPs in a sample-efficient and failure-reducing fashion, based on our definition of safety.
	\end{itemize}
	We hope that the combination of our definition of safety and our algorithm can pave the way for a new class of algorithms for safety-constrained MDPs, not based on the policy-gradient approach of CPO.
	\paragraph{Outline} The rest of the thesis is structured as followed. In Chapter~\ref{chap:prelim}, we introduce the formalism of MDPs and CMDPs and\emph{ viability theory}, upon which our definition of safety largely relies. Chapter~\label{chap:safety from viability} presents the main theoretical definitions on results: safe policies and their characterizations, the strong duality theorem that connects the cost-constrained formulation with the reward shaping one, and the limitations of that theorem. Finally, we describe and evaluate our algorithm in Chapter~\ref{chap:benchmark}.