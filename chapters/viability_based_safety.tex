\chapter{A viability-based characterization of safety} \label{chap:safety from viability}
This chapter establishes the key results of this work on safety and develops intuition on how it should or should not be enforced in practice. Safety is defined as in Section~\ref{sec:setting}, while Section~\ref{sec:safe policies stabilize sv} provides an equivalent practical characterization based on viability. Section~\ref{sec:strong duality} establishes theoretical guarantees of reward-shaping methods. Finally, Section~\ref{sec:parameterization} provides a counterexample to discuss how policy parameterization impairs safety guarantees, even though this may not be visible on the total return function.

\section{Problem setting} \label{sec:setting}
We consider a deterministic Markov decision process $(\S, \A, f, R)$ with unknown dynamics and a finite state-action space $\Q = \S\times\A$. The trajectories and the return are defined as:
\begin{align}
	S_{t+1} & = f(S_t, A_t)\\
	S_0 & \sim \mu,\\
	G_t & = \sum_{u = 0}^\Tf \gamma^uR_{t+u},
\end{align}
where $\gamma < 1$ if $\Tf = \infty$.\par
We are given a failure set $\SF\subset\S$ composed of states that the agent should not explore. The failure set is assumed to be absorbing\footnote{This assumption is not critical to the results presented here, but it eases their formulation.}: the agent needs to be reset after visiting \SF.\par
The combination of the failure set and the dynamics naturally give rise to the viability kernel \SV~,the viable set \QV~and the critical set \QC~of the system. We assume that \SV~and \QV~are not empty: if they are, any policy reaches the failure set in finite time, and so there are no safe policies. In order for safe policies to exist, we also assume that $\support(\mu) \subset \SV$: we explain why in Section~\ref{sec:safe policies characterization}.\par
We can now define safe policies as in~\cite{heim2020learnable}:
\begin{definition}[Safe policy]
	A policy $\pi \in \Pi$ is\emph{ safe} if:
	\begin{equation}
		\P_\pi\left[\bigcup_{t=0}^\Tf\{S_t\in\SF\}\right] = 0. \label{eq:safety def}
	\end{equation}
\end{definition}
This definition of safety belongs to the broad class of\emph{ chance constraints}, which is a standard way of defining safety in RL~\cite{geibel2005risk}~\cite{paternain2019safe}. In our case, this chance constraint is sharp: we do not allow any probability of visiting the failure set. This is quite restrictive, and the existence of policies satisfying it is not straightforward.\par
The safe learning problem can now be stated as finding a safe policy that maximizes the expected return:
\begin{equation}
	\begin{aligned}
		\underset{\pi\in\Pi}{\text{maximize}} & \quad\expected_\pi\left(G_t\right),\\
		\text{s.t.} & \quad \P_\pi\left[\bigcup_{t=0}^\Tf\{S_t\in\SF\}\right] = 0.
	\end{aligned} \label{eq:chance constrained problem}
\end{equation}
This optimization problem can be reformulated as a CMDP with a cost constraint. This is a simple consequence of the following lemma, whose proof is elementary yet insightful:
\begin{lemma}
	For any policy $\pi\in\Pi$, it holds that:
	\begin{equation}
		\pi~\text{is safe}~\iff~\rho_\pi(\mu)~\eqdef~\expected_\pi\left[\sum_{t=0}^\Tf\gamma^t\indicator_\SF(S_{t+1})\right] = 0.
	\end{equation}
\end{lemma}
The quantity $\rho_\pi(\mu)$ is called the\emph{ discounted risk} of the policy~\cite{geibel2005risk}. It mainly depends on the dynamics and the initial state distribution $\mu$. In the following, we will only write it $\rho_\pi$ and drop the explicit dependency in $\mu$.
\begin{proof}
	For any policy $\pi$, $\pi$ is safe if, and only if, for all $t,~\P_\pi(S_t\in\SF) = 0$. We already have $\P_\pi(S_0\in\SF) = 0$, since $\support(\mu)\subset\SV$. Moreover, for $t>0$, we clearly have $\P_\pi(\indicator_\SF(S_t) = 1) = \P_\pi(S_t\in\SF)$. Finally, $\indicator_\SF(S_t)$ is a Bernoulli variable, and so $\P_\pi(\indicator_\SF(S_t) = 1) = \expected_\pi\left[\indicator_\SF(S_t)\right]$. Consequently, $\pi$ is safe if, and only if, for all $t>0,~\expected_\pi\left[\indicator_\SF(S_t)\right] = 0$. The result follows immediately.
\end{proof}
So, problem~\eqref{eq:chance constrained problem} can be reformulated as:
\begin{align}
		\underset{\pi\in\Pi}{\text{maximize}} & \quad\expected_\pi\left(G_t\right), \label{eq:cost constrained problem objective}\\
		\text{s.t.} & \quad \rho_\pi = 0. \label{eq:cost constrained problem constraint}
\end{align}
\paragraph{Guarantees during training} In this work, we do not focus on solving problem~\eqref{eq:cost constrained problem objective}--\eqref{eq:cost constrained problem constraint} while guaranteeing that constraint~\eqref{eq:cost constrained problem constraint} is satisfied during training, which is hopeless in the model-free setting. Instead, we provide a practical characterization of safety and use it to show that penalized methods find asymptotically safe optimal policies. In Chapter~\ref{chap:benchmark}, the algorithms we propose still do not ensure constraints satisfaction at every step, but learn the constraint~\eqref{eq:cost constrained problem constraint} in a sample-efficient manner.

\section{Safe policies stabilize the viability kernel} \label{sec:safe policies stabilize sv}
Because our definition of safety puts a sharp constraint on policies - no probability mass is allowed on failing -, safe policies are the ones that stabilize the viability kernel, as explained in this section. This result of viability theory has an interesting consequence in RL: it provides a simple characterization of safe policies.

\subsection{Reach of a policy}
We define here the reach of a policy:
\begin{definition}[Reach]
	The\emph{ reach} of a policy $\pi\in\Pi$ is the set $\reach_\pi(\mu)\subset\S$ defined as:
	\begin{equation}
		\reach_\pi(\mu) = \left\{s\in\S~\middle|~\P_\pi(\exists t, S_t = s) > 0\right\}. \label{eq:reach}
	\end{equation}
\end{definition}
The reach of a policy is the set of states that are reached with a nonzero probability by following this policy. An illustration of this object is given in Figure~\ref{fig:reach example}. Like the risk $\rho_\pi$, the reach largely depends on the initial state distribution\footnote{In the following, we will not explicitly write this dependency and will use the notation $\reach_\pi$.}. The reach as we define it is different from the standard notion of\emph{ forward reachable set}~\cite{bansal2017hamilton}, which is the set of states that can be reached with a well-chosen control input. As a matter of fact, the reach of a policy is a subset of the forward reachable set of the support of the initial state distribution. Note that, from the first definition of safety, a policy $\pi$ is safe if and only if $\reach_\pi(\mu) \cap\SF = \emptyset$.
\begin{figure}
	\centering
	\includegraphics[width=0.6\textwidth]{reach}
	\caption{The reaches of two policies on the hovership example. The policy $\pi_1$ always applies one thruster, and its agent is initialized in $s=1$ or $s=3$ with equal probability. The policy $\pi_2$ applies one or two thrusters with equal probability, and its agent is always initialized in $s=2$. The reach is shown in light blue. Note that $\pi_2$ is safe, since $\reach_{\pi_2} \subset\SV$, whereas $\pi_1$ is not.}
	\label{fig:reach example}
\end{figure}
\subsection{Safe policies characterization} \label{sec:safe policies characterization}
This enables us to state the following theorem:
\begin{theorem}[Characterization of safe policies]
	For any policy $\pi\in\Pi$, it holds that:
	\begin{equation}
		\pi\text{ is safe}~\iff~\reach_\pi\subset\SV. \label{eq:safe policies stabilize sv}
	\end{equation}
	\label{thm:safe policies stabilize sv}
\end{theorem}
\begin{proof}
	This theorem is a direct consequence of the definition of the viability kernel as the largest controllable invariant set $K \subset\S$ such that $K \,\cap\,\SF=\emptyset$~\cite[Definition\,4.1.1]{aubin2011viability}. Indeed, the reach is clearly invariant under the policy $\pi$. Moreover, we have already pointed out that a policy $\pi$ is safe if, and only if, $\reach_\pi\cap\SF = \emptyset$: this is a direct consequence of the definition of a safe policy. So, $\reach_\pi$ is a controllable invariant subset of $\S\setminus\SF$ if, and only if, $\pi$ is safe: since the viability kernel is the largest of such sets, the result follows immediately.
\end{proof}
We can restate this theorem in a more practical way by using the critical set:
\begin{corollary} \label{clry:safe policies on qc}
	Let $\pi\in\Pi$. The policy $\pi$ is safe if, and only if:
	\begin{equation}
		\forall\,(s, a)\,\in\, \QC \cap (\reach_\pi\,\times\,\A),~\pi(a|s)~=~0. \label{eq:safe policies on qc}
	\end{equation}
\end{corollary}
\begin{proof}
	Let $\pi\in\Pi$. Assume $\pi$ is safe, and consider $(s, a)\,\in\, \QC \cap (\reach_\pi\,\times\,\A)$. Since $(s, a) \notin \QV$, then Theorem~\ref{thm:safe policies stabilize sv} ensures that $s' = f(s, a)\notin\reach_\pi$. Now, pick $t$ such that $\P_\pi(S_t = s) > 0$. We have $0 = \P_\pi(S_{t+1} = s') \geq \pi(a|s)\cdot\P_\pi(S_t = s) \geq 0$. This shows $\pi(a|s) = 0$.\par
	Conversely, assume that $\reach_\pi\not\subset\SV$, and let $s\in\reach_\pi\setminus\SV$. Consider a trajectory $(s_0, a_0, s_1, a_1, \hdots)$ such that:
	\begin{itemize}
		\item there exists a time $t$ such that $s_t = s$;
		\item for all $u < t, \pi(a_u|s_u) > 0$.
	\end{itemize}
	Such a trajectory exists since $s\in\reach_\pi$. Consider the set \mbox{$\mathcal{U} = \{u\in\mathbb{N}~|~s_u\notin\SV\}$}. This set is not empty, so it has a smallest element $u = \min\,\mathcal{U}$. Since $s_0\in\support(\mu)$ and $\support(\mu)\subset\SV$, we have $u \geq 1$. We also have $u\leq t$. Now, we show that $(s_{u-1}, a_{u-1}) \in \QC$. Indeed, $s_u\notin\SV$, by definition of $u$. Yet, $s_{u-1}\in\SV$, still by definition of $u$ and because $u \geq 1$. It follows from the definition of \QC~that $(s_{u-1}, a_{u-1})\in\QC$. Since we also clearly have $(s_{u-1}, a_{u-1})\in\reach_\pi \times \A$, we have found $(s_{u-1}, a_{u-1}) \in \QC\cap(\reach_\pi \times \A)$ such that $\pi(a_{u-1}|s_{u-1}) > 0$. This proves that $\reach_\pi\not\subset\SV$ implies the negation of equation~\eqref{eq:safe policies on qc}, and concludes the proof.
\end{proof}
\subsubsection{Discussion}
\paragraph{Safety does not constrain the policy everywhere} Corollary~\ref{clry:safe policies on qc} shows that safety does not constrain the policy everywhere. Indeed, it makes no statement on how actions should be picked in states that cannot be reached. Of course, the condition of equation~\eqref{eq:safe policies on qc} can still be enforced on the whole critical set \QC, but this is only a sufficient condition for safety.

\paragraph{A classical MDP in a subset of \Q} Policies that satisfy the safety constraint are exactly the ones that effectively prevent the agent from exploring a whole region of the state-action space (namely, $\Q\setminus\QV$). Moreover, any time the agent picks an action in \QV, it ends up in \SV again: the viability kernel is\emph{ positively invariant} under safe policies. Hence, it is possible to see the safety-constrained CMDP~\eqref{eq:chance constrained problem} as a classical MDP with a modified state space $\SV$ and a state-dependent action space $\A(s) = \{a\in\A~|~(s, a)\in\QV\}$. This consideration hints at the fact that most tools specific to MDPs (such as value functions) can actually be used for this CMDP. The main point of Section~\ref{sec:strong duality} is to formalize this intuition.

\paragraph{Safety and initial state distribution} The existence of safe policies is largely conditioned by the initial state distribution $\mu$. Its support must indeed be included in the viability kernel for safe policies to exist, since we always have $\support(\mu) \subset \reach_\pi$. This formalizes the intuition that safety cannot be ensured if the agent is initialized in unviable states. If this condition is not satisfied, then the definition of safety needs to be changed, for example by allowing a non-zero probability of failure.

\paragraph{The viability kernel is a natural safe set} Some traditional definitions of safety as a chance constraint either rely on a safe set~\cite{paternain2019safe} or on a set of states from which the agent should avoid failure~\cite{geibel2005risk}. These sets are typically chosen based on system knowledge, and correspond to states from where it is known that a safe behaviour exists. As it can be seen in the examples of~\cite{paternain2019safe} and~\cite{geibel2005risk}, the underlying goal is still to avoid a failure set, whether it is explicitly named or not. Theorem~\ref{thm:safe policies stabilize sv} establishes that, once the failure set is defined, the viability kernel is a natural choice for such a safe set. It emphasizes that - at least in the $0$--risk setting - the only\emph{ true} degree of freedom the engineer has is defining what failure is: the most general safe set the agent should stay in~-~the viability kernel - is then an emerging property of the dynamics. And even though this viability kernel is generally unknown, it can be learned~\cite{heim2020learnable}.

\section{Theoretical guarantees on reward shaping} \label{sec:strong duality}
A common way of implementing safety constraints in practice is through reward shaping, by incurring penalties when the agent fails. It is known~\cite{altman1999constrained}~\cite{paternain2019safe} that there always exists a value for the penalty so the learned policy is safe and optimal. However, finding this value is highly non-trivial, and often relies on specialized\emph{ primal-dual} algorithms~\cite{chow2017risk}~\cite{paternain2019safe}. We show here that the set of penalties ensuring optimality and safety for the original problem contains an upper-unbounded interval. This result is stronger than what can be found in~\cite{altman1999constrained} or~\cite{paternain2019safe}, and was derived independently.

\subsection{The penalized problem}
Formulating RL algorithms to solve problem~\eqref{eq:cost constrained problem objective}--\eqref{eq:cost constrained problem constraint} is generally hard (see Section~\ref{sec:cmdps preliminaries}). Thus, safety constraints are often enforced in practice by solving the\emph{ penalized problem} instead:
\begin{equation}
	\underset{\pi\in\Pi}{\text{maximize}} \quad\expected_\pi\left[\sum_{t=0}^\Tf\gamma^t\left(R_{t+1} - p\cdot\indicator_\SF(S_{t+1})\right)\right], \label{eq:penalized problem}
\end{equation}
where $p\in\mathbb{R}$ is the\emph{ penalty}. The penalized problem is a variant of the original MDP problem~\eqref{eq:mdp problem} where the reward function has been changed: a penalty is subtracted to the reward when visiting failure states. The penalized problem is unconstrained, and for $p=0$, it boils down to the original MDP problem~\eqref{eq:mdp problem}. \par
Since the penalized problem is unconstrained, we can define its optimal value function (equation~\eqref{eq:optimal state value}):
\begin{equation}
	\forall p\in\mathbb{R},~V_p\,:\,s\in\S\mapsto~\max_\pi~\expected_\pi\left[\sum_{t=0}^\Tf\gamma^t\left(R_{t+1} - p\cdot\indicator_\SF(S_{t+1})\right)\right]. \label{eq:penalized value}
\end{equation}
The penalized problem can be reformulated as follows:
\begin{equation}
\underset{\pi\in\Pi}{\text{maximize}} \quad\expected_\pi\left[G_t\right] - p\cdot\rho_\pi. \label{eq:penalized problem reformulation}
\end{equation}
The effect of penalizing agent failures is thus to put a cost on unsafe policies, and the value of $p$ represents \enquote{how much} it costs to break that constraint. By doing so, we hope that the agent will not only learn a safe policy, but also that this policy is optimal for the original constrained problem~\eqref{eq:cost constrained problem objective}--\eqref{eq:cost constrained problem constraint}. Finding a value of $p$ such that this is true is called\emph{ penalty scaling}. It is underlined in~\cite{garcia2015comprehensive} that there is in general no guarantee that the penalty can be scaled. Indeed, the policy learned by solving~\eqref{eq:penalized problem reformulation} may be unsafe, or on the contrary overly conservative: too high a penalty may result in suboptimal behaviour. The next section shows that such concerns are, in fact, not justified.

\subsection{The penalized problem solves the constrained one}
%In this section and from now on, we assume that $\support(\mu) = \SV$. This assumption is not critical to the results we state here, but it considerably eases their formulation. 
We first introduce the constrained value function before stating our main theoretical result.

\subsubsection{Constrained value function}
We can also define a optimal value functions for the CMDP~\eqref{eq:cost constrained problem objective}--\eqref{eq:cost constrained problem constraint} as follows:
\begin{definition}
	The\emph{ constrained value functions} of the problem~\eqref{eq:cost constrained problem objective}--\eqref{eq:cost constrained problem constraint} are:
	\begin{alignat}{7}
	V^c\,&:&\, &s\in\SV& &\mapsto&~\max_\pi & \quad\expected_\pi\left[G_t~\middle|~S_t = s\right], \label{eq:constrained state value}\\
	& & & & & &\text{s.t.} & \quad \expected_\pi\left[\sum_{t=0}^\Tf\gamma^t\indicator_\SF(S_{t+1})~\middle|~S_t = s\right] = 0, \label{eq:constrained state value constraint}\\
	Q^c\,&:&\,(&s, a)\in\QV& &\mapsto& V^c&\left[f(s, a)\right]. \label{eq:constrained stateaction value}
	\end{alignat}
\end{definition}
\paragraph{Domains of definition} The constrained state value function is defined on \SV~only. Indeed, the constraint in the definition of the function is not feasible if $s\notin\SV$: this is a simple consequence of Theorem~\ref{thm:safe policies stabilize sv} in the case where $\support(\mu) = \{s\}$. The constrained state-action value function is also well-defined, since any state-action in \QV~maps in \SV.
\paragraph{Interpretation} Let us anticipate a bit on Theorem~\ref{thm:strong duality} and provide intuition on constrained value functions. They have exactly the same interpretation as optimal value functions for unconstrained MDPs: the value of a state (resp. state-action) represents the remaining return that can be collected from that state (resp. state-action) by following an optimal policy. 

\begin{remark}
	As we emphasized it in Section~\ref{sec:cmdps preliminaries}, CMDPs cannot be solved in general by value methods. Hence, constrained value functions are not a standard tool to study CMDPs. The definition of safety as a $0$-risk chance constraint makes the CMDP~\eqref{eq:cmdp problem} behave\emph{ as if} it were a standard MDP evolving in a subset of the state-action space (namely, \QV), as stated by Corollary~\ref{clry:safe policies on qc}. This very peculiar property makes constrained value functions relevant here.
\end{remark}

\begin{remark}[Conditioning of the constraint]
	It can be argued that the constraint~\ref{eq:constrained state value constraint} should not be conditioned on $\{S_t = s\}$, since the resulting constraint is less demanding than~\eqref{eq:cost constrained problem constraint}. This is true and general. However, there is no universal definition of constrained value functions to the best of our knowledge, so we choose the definition that allows an intuitive interpretation.
\end{remark}

\subsubsection{Penalized policies are safe and optimal}
We are now ready to state the two main results of this section.
\begin{theorem} \label{thm:strong duality}
	The following two conditions hold:
	\begin{alignat}{3}
		\exists p^*\in\mathbb{R},~\forall p > p^*,~&\forall s\in\SV,&~&V_p(s) = V^c(s),\label{eq:strong duality sv}\\
		&\forall s\in\S\setminus\SV,&~&V_p(s)\goesto{p\to\infty}-\infty. \label{eq:strong duality su}
	\end{alignat}
\end{theorem}
The proof of Theorem~\ref{thm:strong duality} can be found in Appendix~\ref{chap:proof strong duality}. We immediately state a corollary of equal importance:
%\begin{corollary} \label{clry:penalized solves constrained}
%	For all $p\in\mathbb{R}$, let $\pi_p$ be the solution of the penalized problem. There exists a threshold $p^*\in\mathbb{R}$ such that, for all $p > p^*$:
%	\begin{itemize}
%		\item $\pi_p$ is safe and is a solution of the constrained problem~\eqref{eq:chance constrained problem};
%		\item for any policy $\pi^*$ solution of~\eqref{eq:chance constrained problem}, we have $\pi^*|_\SV = \pi_p|_\SV$. That is, the solution of~\eqref{eq:chance constrained problem} is unique on the viability kernel,
%	\end{itemize}
%	where the state-restriction $\pi|_\SV$ of a policy $\pi$ to the viability kernel is defined as
%	\begin{equation}
%		\pi|_\SV\,:\,(s, a)\in\SV\times\A\,\mapsto\,\pi(a|s).
%	\end{equation}
%\end{corollary}
\begin{corollary} \label{clry:penalized solves constrained}
	There exists a threshold $p^*\in\mathbb{R}$ such that, for all $p > p^*$ any solution of the penalized problem~\eqref{eq:penalized problem} is safe and optimal for the constrained problem~\eqref{eq:chance constrained problem}.
\end{corollary}
\begin{proof}
	Let $p^*$ be a threshold as given by Theorem~\ref{thm:strong duality}. Since \Q~is finite, we can assume without loss of generality that:
	 \begin{equation}
	 \forall p > p^*,~\max_{s\in\S\setminus\SV}V_p(s) < \min_{s\in\SV}V^c(s). \label{eq:proof corollary 2 v_p su}
	 \end{equation}
	 For all $p\in\mathbb{R}$, let $\pi_p$ be an optimal solution of the penalized problem~\eqref{eq:penalized problem}.  We first show that, for all $p>p^*,~\pi_p$ is safe. From the established theory of MDPs~\cite{sutton2018reinforcement}, we know that $\pi_p$ satisfies the optimal Bellman equation on the states it can reach. Hence, for all $s\in\reach_{\pi_p}$:
	\begin{equation}
		\pi_p(a|s) > 0~\iff~a \in \argmax_{a'}\,V_p(f(s, a')),\label{eq:proof corollary 2 pi_p}
	\end{equation}
	By combining equations~\eqref{eq:strong duality sv} and \eqref{eq:proof corollary 2 pi_p}--\eqref{eq:proof corollary 2 v_p su}, we get that:
	$$
		\forall (s, a) \in \QC\cap(\reach_{\pi_p}\times\A),~\pi_p(a|s) = 0,
	$$
	which means that $\pi_p$ is safe according to Corollary~\eqref{clry:safe policies on qc}.\par
	Now, the following holds:
	\begin{equation}
	\begin{alignedat}{5}
		\expected_{\pi_p}\left[G_t\right] & =~&\expected_{\pi_p}&\left[G_t\right] - p\cdot\rho_{\pi_p}, & &\quad\text{since}~\rho_{\pi_p} = 0,\\
		& =~&\max_\pi&~\expected_{\pi_p}\left[G_t\right] - p\cdot\rho_{\pi_p},& &\quad\text{since}~\pi_p~\text{is optimal},\\
		& \geq~& \max_\pi&~\expected_{\pi_p}\left[G_t\right],& &\quad\text{by the weak duality lemma~\ref{lemma:weak duality}},\\
		&		& \text{s.t.}&~\rho_\pi = 0,& &\\
		& \geq~& \expected_{\pi_p}&\left[G_t\right], & &\quad\text{since}~\pi_p~\text{is safe}.
		\end{alignedat}\label{eq:proof corollary 2 duality}
	\end{equation}
	Consequently, all of the inequalities are equalities, and so $\pi_p$ achieves the optimal value of the constrained problem. This concludes the proof.
\end{proof}

\paragraph{Scaling the penalty is easy} This result is stronger than what it generally found in the literature. In~\cite{altman1999constrained} or~\cite{paternain2019safe}, the only guarantee is that there exists a penalty for which the penalized policy is safe and optimal. The fact that\emph{ any} penalty greater than $p^*$ proves that it is unnecessary to use primal-dual algorithms that also optimize on the value of the penalty. However, as Section~\ref{sec:parameterization} emphasizes it, Theorem~\ref{thm:strong duality} only holds when optimizing on the set of\emph{ all} policies in the penalized problem.

\paragraph{Bounds on the minimal value of $p^*$} \label{sec:gamma p trade off} The theorem does not provide any information on what is the minimal value of $p^*$. It turns out that conservative estimates can be given: for example, it can be shown\footnote{This is just given as an example and is not proven here.} that in the case of a deterministic reward $R$, the quantity: 
$$
	\frac{1}{\gamma^{|\S\setminus\SV|}}\max_{(s,a)\in\Q\setminus\QV}R(s, a) + (1 - \gamma)\min_{(s, a)\in\QV}R(s, a),
$$
is a value of $p^*$ that satisfies Theorem~\ref{thm:strong duality}. The most remarkable thing about this threshold is that is grows as $\gamma$ gets close to $0$. This makes a lot of sense: as $\gamma$ goes to $0$, the agent is more and more oblivious to long-term rewards. Hence, the penalty needs to be scaled accordingly.

\paragraph{Influence of the penalty on the learning} Whether or not an agent trained on the penalized reward finds the optimal policy (which is safe, according to Theorem~\ref{thm:strong duality}) largely depends on what learning algorithm is used. In particular, high values of the penalty may break some algorithms by making them numerically unstable, or very sample-inefficient. Since theoretical guarantees hold as long as $p > p^*$, the algorithm's sensitivity to the penalty should be the main drive to choose $p$. According to the numerical proof of concept that we present in the next paragraph, Q-Learning does not seem to be very sensitive to the value of $p$: the number of iterations required to find the optimal policy on the hovership example does not noticeably vary with $p$.

\subsubsection{Application to the hovership example} \label{sec:hovership example}
We describe here a numerical experiment showing how Theorem~\ref{thm:strong duality} and Corollary~\ref{clry:penalized solves constrained} can be used in practice. The environment is a variation of the hovership presented in Section~\ref{sec:hovership} with larger state and action spaces. The reward is an affine function of the state: it is $0$ when $s=2$, and $10$ when $s=0$, to make the agent go as low as possible. The discount rate was set to $\gamma = 0.3$. We chose such a low value for $\gamma$ to emphasize that the value of $p^*$ can grow very quickly depending on how the problem is parameterized.

\paragraph{Solving the penalized problem} The algorithm used to solve the penalized problem is Q-Learning~\cite{watkins1992q}, with an $\varepsilon$-greedy policy with $\varepsilon = 0.1$ and a step size of $0.6$.

\paragraph{Solving the constrained problem} We also solved the constrained problem with an iterative method to illustrate how the constrained problem can be solved directly by only looking for policies that satisfy the constraint in Corollary~\ref{clry:safe policies on qc}. The algorithm we used for that is a modified version of Q-Learning with an $\varepsilon$-greedy exploration policy:
\begin{itemize}
	\item with probability $1-\varepsilon$: the agent picks the action that maximizes its current estimate of $Q^c$;
	\item the agent picks any other\emph{ viable} action with uniform probability.
\end{itemize}
Note that this is exactly the Q-Learning algorithm where the set of actions is constrained to viable actions. We call this algorithm \enquote{constrained Q-Learning}. Of course, it requires the viable set to be known.

\paragraph{Results} The results are presented in Figures~\ref{fig:hovership values comparison} and~\ref{fig:hovership q values}. The statements of Theorem~\ref{thm:strong duality} are best seen on Figure~\ref{fig:hovership values comparison}: the values of all states decrease as the penalty increases. When the penalty exceeds a certain threshold (between $15$ and $200$), the penalized values in the viability kernel stop decreasing and stabilize at\emph{ exactly} the constrained values. Outside of the kernel, the values go arbitrarily low. The resulting policies can be examined on Figure~\ref{fig:hovership q values}. When the penalty is too low, the resulting greedy policy is unsafe. As the penalty grows, taking unviable actions is deemed less and less interesting, and the policy becomes safe when the penalty is big enough. Then, the penalized agent only considers actions in \QV, and it finds the optimal policy of the constrained problem.
\begin{figure}
	\centering
	\includegraphics[width=0.6\textwidth]{constrained_value/small_penalties_final_values.pdf}
	\caption{The optimal value function for different values of the penalty on the hovership example. The viability kernel is $\SV=\{6, 7, 8, 9, 10\}$. The constrained value function is plotted in green, and is equal to the penalized functions for the biggest penalties. The value at $s=0$ is an artifact and is not meaningful. The policy for $p=15$ is not safe, since $V_{15}(5) > V_{15}(6)$ and $s=5$ is not viable. The lowest threshold $p^*$ is somewhere between $15$ and $200$.}
	\label{fig:hovership values comparison}
\end{figure}
\begin{figure}
	\centering
	\begin{subfigure}{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{constrained_value/constrained_final_Q-Values.pdf}
		\caption{Constrained}
		\label{fig:hovership q values:constrained}
	\end{subfigure}
	\begin{subfigure}{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{constrained_value/penalized_0_final_Q-Values.pdf}
		\caption{$p=0$}
		\label{fig:hovership q values:0}
	\end{subfigure}
	\newline
	\begin{subfigure}{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{constrained_value/penalized_15_final_Q-Values.pdf}
		\caption{$p=15$}
		\label{fig:hovership q values:15}
	\end{subfigure}
	\begin{subfigure}{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{constrained_value/penalized_200_final_Q-Values.pdf}
		\caption{$p=200$}
		\label{fig:hovership q values:200}
	\end{subfigure}
	\caption{The optimal state-action value function for the constrained problem and different values of the penalty. The numbers are the value of the state-action. The color rectangles correspond to the state-action on their lower-left vertex. On Figure~\ref{fig:hovership q values:constrained}, the viable set is horizontally striped, the failing state-actions are crossed, and state-actions failing in more than one step are diagonally striped. The function is not defined outside of the viable set, hence the value $0$. On all figures, the $0$ values at $s=0$ are artifacts and should be ignored. The two small differences in the values in the viable set on Figures~\ref{fig:hovership q values:constrained} and~\ref{fig:hovership q values:200} are interpreted as the estimates not having quite converged yet.}
	\label{fig:hovership q values}
\end{figure}
\section{The price of parameterization} \label{sec:parameterization}
In many of the real-world applications of RL, the state-action space is enormous. Because of that, finding an optimal policy becomes an impossible task, since it would require not only lots of computational power and memory, but also unreasonable amounts of data. This last point is critical when applying RL to dynamical systems, where collecting data is risky and expensive. For these reasons, we restrict the policies to a set of\emph{ parameterized policies}, and only look for the parameters that maximize the expected return. Ensuring the safety of the learned parameterized policy is also critical, like it was in the unparameterized setting. An important question is then: do the guarantees on penalty scaling still hold when parameterizing the policies? Is it still reasonable to solve the easier penalized problem to learn safe policies? The short answer is no, and Section~\ref{sec:unsafe parameterized} gives a counterexample showing it. In Section~\ref{sec:value unreliable}, we emphasize on another example that the resulting\emph{ duality gap} between the penalized and the constrained problem is a very poor indicator of whether the policy learned by penalized methods is close to being safe.

\begin{figure}
	\centering
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=0.6\textwidth]{counterexample}
		\caption{First example. Here, $\theta_\text{max} = \frac{\sqrt{5}-1}{2}$.}
		\label{fig:counterexample 1}
	\end{subfigure}
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=0.6\textwidth]{counterexample_2}
		\caption{Second counterexample, with hyperparameter $\alpha$.}
		\label{fig:counterexample 2}
	\end{subfigure}
	\caption{Two counterexamples. The agent is initialized in $s=0$. Once it reaches $s=1, 2$ or $3$, it cannot go back. The state $s=3$ is considered a failure. The policy is parameterized by the scalar $\theta\in[0, \theta_{\text{max}}]$, where $\theta_\text{max}$ depends on the parameterization.}
	\label{fig:counterexamples}
\end{figure}

\subsection{Penalized parameterized policies are unsafe} \label{sec:unsafe parameterized}
In this section, we develop a counterexample showing that there is in general no way of choosing a\emph{ finite} value of the penalty to get a safe, optimal policy by solving the penalized problem.

\subsubsection{Problem setting} The MDP we consider is described in Figure~\ref{fig:counterexample 1}. The policies are parameterized by a scalar parameter $\theta\in[0,\theta_{\text{max}}]$, with $\theta_\text{max} = \frac{\sqrt{5}-1}{2}$:
$$
	\pi_\theta(a|s) = \begin{cases}
		\theta,\quad\text{if}~a=a_1,\\
		1 - \theta - \theta^2,\quad\text{if}~a=a_2,\\
		\theta^2,\quad\text{if}~a=a_3.\\
	\end{cases}
$$
If $s\neq0$, all the actions have the same effect and keep the agent in its current state. The constrained and the penalized problems can respectively be rewritten as follows:
\begin{align*}
	\underset{0\leq\theta\leq\theta_\text{max}}{\text{maximize}}&~(1-\gamma)\cdot\theta,\\
	\text{s.t.} &~\theta^2 = 0,\\
	\underset{0\leq\theta\leq\theta_\text{max}}{\text{maximize}}&~(1-\gamma)\cdot\theta\cdot\left[1 - p\cdot\theta\right],
\end{align*}
The question we answer now is: does there exist a scaling of the penalty $p$ for which the solution of the penalized problem is safe and has the same value as the constrained problem?

\subsubsection{Solution} The constrained problem obviously has only one feasible parameter: $\theta~=~0$. The return achieved by the corresponding policy is simply $G^c = 0$. For a given penalty $p$, the optimal parameter for the penalized problem is $\theta_p = \frac{1}{2p}$. This policy achieves a return of $G_p = \frac{1-\gamma}{4p}$. The duality gap is then:
$$
	\Delta = G_p - G^c = \frac{1-\gamma}{4p}.
$$
We can clearly see that there exists no finite value of the penalty that makes $\Delta = 0$: the penalty cannot be scaled. What's more, for any value of the penalty, the optimal policy for the penalized problem has a probability of failing equal to $\frac{1}{2p}>0$: the policy is not safe.

\subsubsection{Discussion} 
\paragraph{The penalized problem gives unsafe policies} This counterexample demonstrates that the theoretical guarantees of Theorem~\ref{thm:strong duality} are lost when parameterizing the policy. In general, the penalized and the constrained problems have different expected returns, and the optimal policy of the penalized problem is not safe. However, weak duality (Lemma~\ref{lemma:weak duality}) still holds: this is shown in~\cite{paternain2019safe}. Note however that, in the example, the guarantees still hold asymptotically when $p\to\infty$. We suspect that this is always true, but do not make any claims about this.

\paragraph{Comparison with the unparamaterized problem} We have compared here the return of the\emph{ unparameterized} problems. In~\cite{paternain2019safe}, the authors compare the return of the parameterized penalized problem with the one the unparameterized constrained problem, and they bound this suboptimality gap with the approximation error of the parameterization. Our result is of different interest: it answers the question of how the problem of finding safe policies should be solved once the parameterization is chosen.

\subsection{The duality gap is a poor indicator of the risk} \label{sec:value unreliable}
One of the main results of~\cite{paternain2019safe} is showing that the difference between the optimal expected returns of the penalized and the constrained problems is bounded by the approximation error of the policy. The underlying motivation is guaranteeing when parameterizing policies can be done without sacrificing too much performance. We argue here on a counterexample that guaranteeing a small gap in the return does\emph{ not} give any guarantees on the underlying safety of the policy. Our example shows that, for a given duality gap, the hyperparameters of the parameterization and the penalty can be chosen such that the resulting policy can have an arbitrarily high risk of failure. This is of particular importance since tuning the penalty and the rest of the hyperparameters is often done at the same time, and can result in risky policies without this being visible on the return signal.

\subsubsection{Problem setting}
The MDP we consider here is described in Figure~\ref{fig:counterexample 2}. The dynamics are the same as in the previous section, but the class of policies is now parameterized by the hyperparameter $\alpha > 1$:
$$
\pi_\theta(a|s) = \begin{cases}
\theta,\quad\text{if}~a=a_1,\\
1 - \theta - \theta^\alpha,\quad\text{if}~a=a_2,\\
\theta^\alpha,\quad\text{if}~a=a_3.\\
\end{cases}
$$
Since $\alpha$ is a hyperparameter, the RL agent does not optimize on it: its only degree of freedom is the parameter $\theta\in[0,\theta_\text{max}]$, with $\theta_\text{max}$ depending on $\alpha$. Once again, the constrained and the penalized problem can be rewritten as\footnote{We omit the $1-\gamma$ factor for clarity since it does not have any impact on the solutions. This is equivalent to choosing $\gamma = 0$.}:
\begin{align*}
\underset{0\leq\theta\leq\theta_\text{max}}{\text{maximize}}&~\theta,\\
\text{s.t.} &~\theta^\alpha = 0,\\
\underset{0\leq\theta\leq\theta_\text{max}}{\text{maximize}}&~\theta - p\cdot\theta^\alpha.
\end{align*}
The expected return of the constrained problem is noted $G^c$, and the one of the penalized problem is $G_p^\alpha$ (the superscript is a notation, and does not represent \enquote{$G_p$ to the power $\alpha$}). The resulting duality gap is noted $\Delta_p^\alpha = G_p^\alpha - G^c$.

\paragraph{Definition of \thetamax} The parameter \thetamax~is the only positive solution to the equation:
\begin{equation}
	\thetamax^\alpha = 1 - \thetamax.\label{eq:theta max}
\end{equation}
This equation comes from the fact that the probability of every transition must be nonnegative.

\subsubsection{Optimal values and parameters}
This problem is simple enough to be solved with pen and paper, and we give the solutions here. The only value of the parameter satisfying the safety constraint is $\theta = 0$, and so the expected return of the constrained problem is once again $G^c = 0$.\par
For a given penalty $p$ and hyperparameter $\alpha$, the penalized expected return is maximal in :
\begin{equation}
	\thetapalpha = \frac{1}{(\alpha p)^\frac{1}{\alpha - 1}}.\label{eq:theta p alpha}
\end{equation}
But recall that the parameter $\theta$ should be lower than \thetamax. Hence, the expected return is:
\begin{equation*}
	G_p^\alpha = \begin{dcases}
		\thetapalpha - p \thetapalpha,&\quad\text{if}~\thetapalpha\leq\thetamax,\\
		\thetamax - p \thetamax^\alpha,&\quad\text{otherwise}.
	\end{dcases}
\end{equation*}
By combining this equation with equations~\eqref{eq:theta max}--\eqref{eq:theta p alpha}, we get the following expression for the duality gap:
\begin{equation}
	\Delta_p^\alpha = \begin{dcases}
	\frac{1 - \frac{1}{\alpha}}{(\alpha p)^\frac{1}{\alpha - 1}}, &\quad\text{if}~\thetapalpha\leq\thetamax,\\
	p\cdot(\thetamax - 1) + \thetamax,&\quad\text{otherwise}.
	\end{dcases} \label{eq:duality gap}
\end{equation}

\subsubsection{Different hyperparameters for a given duality gap}
Now, assume that we aim at parameterizing the problem so the duality gap $\Delta_p^\alpha$ is set to an acceptable value \Deltatarget, corresponding to our tolerance on the suboptimality of the penalized problem compared to the constrained one. What are the acceptable values of $(p, \alpha)$ that achieve this objective? This question can be answered by expressing the nonlinear relation between $p$ and $\alpha$ given by Equation~\ref{eq:duality gap}, where $\Delta_p^\alpha$ is now considered fixed and equal to \Deltatarget. We get:
\begin{equation}
	p = \begin{dcases}
		\frac{1}{\alpha}\left(\frac{1 - \frac{1}{\alpha}}{\Deltatarget}\right)^{\alpha - 1},&\quad\text{if}~\thetapalpha\leq\thetamax,\\
		\frac{\thetamax - \Deltatarget}{1 - \thetamax},&\quad\text{otherwise}.
	\end{dcases}\label{eq:p alpha}
\end{equation}
This relation expresses the value the penalty should have in order to achieve the duality gap \Deltatarget~with the hyperparameter $\alpha$. Recall that \thetamax~also depends on $\alpha$. There may be better better penalties that achieve a lower duality gap: this value is the one for which the tolerance is met. Typically, it can be found by primal-dual algorithms whose stopping criterion depend on the gap.

\subsubsection{The risk cannot be inferred from the duality gap}
For a given choice of $\alpha$ and with the penalty $p$ given by~\eqref{eq:p alpha}, the risk $\rho_{\alpha}$ of the optimal penalized policy is:
\begin{equation}
	\rho_{\alpha} = \begin{dcases}
		\left(\frac{\alpha\Deltatarget}{\alpha - 1}\right)^\alpha,& \quad\text{if}~\thetapalpha\leq\thetamax,\\
		1 - \thetamax,&\quad\text{otherwise}.
	\end{dcases} \label{eq:risk alpha}
\end{equation}
Getting an explicit formula in terms of $\alpha$ is quite hard because of the implicit dependency in the conditioning. The risk can still be evaluated using numerical methods, and is plotted on Figure~\ref{fig:risk plot}. 
\paragraph{Discussion} Figure~\ref{fig:risk plot} shows that the risk can vary a lot depending on the value of $\alpha$. No matter the value of $\Deltatarget > 0$, a poor choice of $\alpha$ can result in agents that have a $50\%$ probability of failing. And, for all values of $\alpha$, these agents achieve\emph{ exactly the same duality gap}: this duality gap is thus a very poor indicator of the risk of the learned policy. When solving safe RL problems by penalizing failures with parameterized policies, \emph{controlling that the resulting duality gap is small is not sufficient to ensure approximate safety of the learned policy}: this risk needs to be evaluated and controlled independently.

\begin{figure}
	\centering
	\includegraphics[width=0.5\textwidth]{risk_counterexample}
	\caption{The risk~\eqref{eq:risk alpha} of the optimal policy when $p$ is chosen to ensure that $\Delta_p^\alpha = \Deltatarget$. For low values of $\alpha$, we have $\theta_{p,\alpha} > \thetamax$: the expression defining the risk in Equation~\eqref{eq:risk alpha} changes, causing the slope break visible on every curve. Then, the risk does not depend on $\Deltatarget$.}
	\label{fig:risk plot}
\end{figure}

\section{Conclusion on the penalization-based approach}

Solving the penalized problem instead of the constrained problem is much easier to implement in practice, but it suffers from a lot of other problems. While it is guaranteed to find safe and optimal policies in the unparameterized setting, all these guarantees vanish whenever the policies are parameterized - which is always the case in real-world applications. These policies can still perform well from the expected return perspective, as demonstrated in~\cite{paternain2019safe}, but giving any guarantees on the safety of the learned policy is a different problem altogether. For applications to real robots, the problem of sample-efficiency is also critical and is absent from this analysis. Indeed, reducing the number of failures is important to protect both the robot and its surroundings. While the agent may be guaranteed to learn a safe policy when failures are penalized, the number of failures required to ensure this condition is not known.\par
Safe policies enjoy a very simple characterization based on the viable set: they are simply policies that only consider actions inside that set. Even though the viable set is generally not known in practice, this characterization opens the door for a new way of solving the CMDP\eqref{eq:chance constrained problem}: only consider policies that stay inside the viability kernel - or inside an estimate of it. The constrained Q-Learning algorithm that we described on the example of the hovership in Section~\ref{sec:hovership example} is a very simple implementation of this consideration.