\chapter{Safely learning values: a benchmark} \label{chap:benchmark}
In this chapter, we derive and benchmark algorithms to learn safe, optimal policies leveraging the theoretical insight developed in the previous chapter. The goal of this benchmark is to test how successful the algorithms are at maximizing the reward while also quickly learning how to avoid failure. We first present the benchmarked algorithms and the set-up in Section~\ref{sec:set-up} before giving the experimental results in Section~\ref{sec:results}.

\section{Experimental set-up} \label{sec:set-up}
We first describe the experimental set-up of this benchmark. Two environments were tested: an extension of the hovership environment presented in Section~\ref{sec:hovership} with a continuous state-action space~\cite{heim2020learnable}, and the spring-loaded inverted pendulum (SLIP) model~\cite{heim2019beyond}. We test the four algorithms presented below on each of these environments. Their performances are evaluated on several metrics: the failure rate of the learned policy, the total reward it collects before the environment terminates, the discrepancy between the true viable set and its estimate (when applicable), and the number of failures during training.

\subsection{Environments}
The proposed methods are tested on different systems, whose dynamics, termination conditions, and rewards are described below. In each case, solving the dynamics involved solving one or many differential equations. The values of the parameters of the dynamics were taken from the code of~\textcite{heim2020learnable}, where the same environments are used for numerical examples. Each of these environments illustrates a different challenge: dealing with large unviable states, and complex dynamics where failing can happen at every step.

\subsubsection{Hovership}
The first system is the hovership environment, which can be parameterized to have a large unviable set.

\paragraph{State-action space and dynamics} The example based on the hovership spaceship from Section~\ref{sec:hovership} is modified with a continuous state-action space. The state $s$ is now the altitude of the spaceship, and is a continuous variable in time. It satisfies the following differential equation:
\begin{equation}
	\dot{s} =- g - 0.75\cdot\tanh\left(s_\text{max} - s\right)\cdot\nabla g + a, \label{eq:hovership dynamics}
\end{equation}
where $g=0.1$, $\nabla g = 1$, and $s_\text{max} = 1$ are parameters, and $a$ is the action. The state is constrained to stay in $[0, s_\text{max}]$. In order to make the dynamics time-discrete, the agent is only allowed to change its action on regularly spaced time steps: the\emph{ control frequency} $f = 1$ \hertz~of the agent is another parameter. Hence, the action $a$ in the dynamics~\eqref{eq:hovership dynamics} is piecewise constant.

\paragraph{Environment termination} The failure set is $\SF = \{0\}$: this corresponds to the ship crashing on the ground. There are two different ways that the environment terminates: either the agent fails, or it manages to take 10 consecutive steps without failing. This value is taken as a heuristic: we estimate that any agent that should fail will fail within this time horizon\footnote{The results of the simulations proved that this is not true: some agents managed to end up in the unviable set after 10 steps without having failed yet. This only had minor consequences on the results, but will be corrected for future experiments.}. The dynamics~\eqref{eq:hovership dynamics} and the parameters were chosen so that a large portion of the state action space is unviable, as it can be seen on Figure~\ref{fig:state action spaces:hovership}.

\paragraph{Reward} The reward is an affine function of the state. It decreases from $10$ for $s=0$ (the lowest altitude) to $0$ for $s=s_\text{max}$ (the highest altitude). The optimal policy is then to go as low as possible, with the maximum possible thrust: this corresponds to reaching the lower-right region of the viable set.

\begin{figure}[t]
	\centering
	\begin{subfigure}{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{state_action_spaces/hovership}
		\caption{Hovership}
		\label{fig:state action spaces:hovership}
	\end{subfigure}
	\begin{subfigure}{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{state_action_spaces/slip}
		\caption{SLIP}
		\label{fig:state action spaces:slip}
	\end{subfigure}
	\caption{The state-action spaces of the two environments. On each figure, the viable set \QV~is in green, unviable state-actions are in yellow, and failing state-actions are in orange. A safe agent only picks actions in the green area.}
\end{figure}

\subsubsection{Spring-loaded inverted pendulum} 
The spring-loaded inverted pendulum (SLIP) is a model commonly used to describe running~\cite{heim2019beyond}. It has hybrid and highly non-linear dynamics, and the complex shape of its viable set make it a challenging task for control.

\paragraph{State-action space and dynamics} The SLIP model is composed of a spring loaded with a mass. The dynamics can be decomposed in three phases: the flight phase, the stance phase, and another flight phase. The summary of the phases can be found on Figure~\ref{fig:slip motion phases}. We use the approach from~\cite{heim2019beyond} to describe this system using a $1$-dimensional state $s\in[0,1]$, representing the normalized altitude of the mass at the flight apex. The control consists in changing the angle of attack of the spring leg and is applied at each flight apex, so one time per cycle. The dynamics are then treated as a discrete-time, nonlinear, and deterministic map with a two dimensional state-action space.

\paragraph{Environment termination} Failure is defined as the mass hitting the ground, or changing direction during the stance phase: it is evaluated on the full state space of the continuous dynamics. The environments can then terminate in two different ways: either the agent fails, or it manages to take $10$ consecutive steps without failing. This is here again a heuristics, but is supported by the fact that any state can be reached from any other state in at least two steps~\cite{zaytsev2015two}. The state-action space and the viable set of the SLIP model can be seen on Figure~\ref{fig:state action spaces:slip}. Note that the shape of the SLIP's viable set make it a much more challenging task for control than the hovership, as failing is possible from every state.

\paragraph{Reward} The reward is here again an affine function of the state, and is maximal when the state is lowest. The dynamics in the lower region of the viable set map back to the upper part of the state space. Depending on the discount factor, the optimal policy is then a trade off between going low (immediate reward) and staying at a low altitude on average (long term reward).

\begin{figure}[t]
	\centering
	\includegraphics[width=0.6\textwidth]{slip}
	\caption{A qualitative trajectory of the SLIP model. The control input $\alpha$ is chosen at the first flight apex. The foot lands, pivots around its contact point, and the system springs again. At the next flight apex, a new angle of attack $\alpha$ can be chosen and a new cycle begins. \figfrom{\cite{heim2019beyond}}.}
	\label{fig:slip motion phases}
\end{figure}

\begin{remark}
	Terminating environments after a certain number of steps is taken is a common practice in RL. However, it can be argued that it breaks the Markov assumption. This is a very legitimate concern, and is a topic of study in the RL community~\cite{pardo2018time}. However, the state-action spaces involved here are sufficiently small so the agent has the time to either reach one of its limit cycles or to fail within the alloted time. Moreover, agents are initialized randomly in the viability kernel: this randomization ensures that no part of the state space is insufficiently explored due to time constraints.
\end{remark}

\subsection{Algorithms}~\label{sec:algorithms}
We implement four different learning strategies to find safe policies. Three of them build up on the work of~\textcite{heim2020learnable} and use the safety measure presented in Section~\ref{sec:safety measure} to estimate the viable set \QV~to favor safe exploration. The main difference between these algorithms is how they select new samples and when they update their estimate of the viable set. These algorithms are compared with Q-Learning with and without penalization. A summary of all the algorithms and their parameterization is available on Table~\ref{tab:algorithms}. The different functions that need to be learned are approximated with Gaussian Processes (GPs)~\cite{williams2006gaussian}.

\begin{table}
	\centering
	\begin{tabular}{l|ccc}
		 & Parameters & Sampling & \LQhat~update\\
		 \thickhline
		Safety-Q switch 
								 & \multiline{$\gamma_\text{opt}, \gamma_\text{caut}$,\\
												 	 $\lambda_\text{caut}, t_\text{switch}$} 
								 & \multiline{From~\cite{heim2020learnable},\\then\\constrained\\Q-Learning}
								 & \multiline{$t\leq\tswitch$:\\Every step\\$t > \tswitch$:\\On failure}\\
		\hline
		Safety Q-Learning 
									& \multiline{$\gamma_\text{opt}, \gamma_\text{caut}$,\\
										$\lambda_\text{caut}$}
									& \multiline{Constrained\\Q-Learning}
									& Every step\\
		\hline
		Soft Safety Q-Learning 
										   & \multiline{$\gamma_\text{opt}, \gamma_\text{caut}$,\\
										   	$\lambda_\text{caut},\gamma_\text{soft}$} 
										   & \multiline{Constrained\\Q-Learning}
										   & \multiline{$a\notin\Qsoft$\\On failure}\\
		\hline
		Penalized Q-Learning & $p$ & Q-Learning & None\\
		\thickhline
		
	\end{tabular}
\caption{The four algorithms and their main parameters and behaviour. All algorithms also have parameters to configure the underlying Q-Learning: probability of not acting greedily $\varepsilon$, step size, discount rate. The \enquote{Constrained Q-Learning} update means that the chosen action in $s$ is $\argmax_{a, (s,a)\in\Qcaut}Q(s, a)$ with probability $1 - \varepsilon$, or another random action $a$ such that $(s, a)\in\Qcaut$ otherwise. The \enquote{Q-Learning} one means the same thing without the state-actions being in \Qcaut.}
\label{tab:algorithms}
\end{table}

\subsubsection{Standard Q-Learning}
The first algortihm that we implement is Q-Learning, and we test it on the penalized and on the unpenalized environments. The unpenalized Q-Learning is not expected to be particularly good, and is just evaluated here as a baseline. However, it is reasonable to expect that Q-Learning on the penalized environment actually learns a safe policy, from Theorem~\ref{thm:strong duality}: Gaussian processes should not suffer from the problems emphasized in Section~\ref{sec:parameterization} since they can, in principle, approximate any function provided that enough data is available. Of course, the required amount of data largely depends on the regularity of the approximated function, and on the hyperparameters of the GP (the kernel, for example). Here, the regularity of the value function largely depends on the value of the penalty: higher penalties mean sharper local variations. Hence, depending on the value of the penalty, the penalized Q-Learning algorithm may be able to learn the optimal safe policy without suffering too much from numerical issues.

\subsubsection{Safety-Q switch}
We make Q-Learning aware of the safety constraints by building up on the algorithm of~\textcite{heim2020learnable} presented in Section~\ref{sec:safety measure}. The idea of \enquote{Safety-Q switch} is to first run this viable set estimation algorithm, and then run a constrained Q-Learning algorithm that only allows the agent to pick actions inside the cautious estimate of the viable set~\Qcaut. Deciding when to switch to Q-Learning is key: it can either be done by evaluating the convergence of the algorithm from~\cite{heim2020learnable}, or after a certain number of iterations. We chose the second method for simplicity: we call this number of iterations the \emph{ switch time}, and note it $t_\text{switch}$. In these examples, this parameter is easy to tune since we have perfect system knowledge and know how many steps the algorithm learning \QV~requires. However, choosing \tswitch~is not straightforward in general. The main advantage of the next algorithms compared to Safety-Q Switch is that they do not have such a parameter.
\paragraph{Theoretical guarantees} The Safety-Q switch algorithm enjoys very strong theoretical guarantees, as long as its estimate of \QV~is correct. Assume that the algorithm from~\cite{heim2020learnable} has found \Qcaut~such that $\QV = \Qcaut$. Then, any policy output by the Safety-Q switch algorithm for $t \geq \tswitch$ is safe: this is a simple consequence of Corollary~\ref{clry:safe policies on qc}. Since the viability kernel is positively invariant under safe policies, the agent never goes out of \QV~ever again. Hence, the agent is simply doing Q-Learning on a stable subset of the state-action space, and so it is guaranteed to asymptotically find the optimal policy as soon as the convergence assumptions of Q-Learning are met~\cite{sutton2018reinforcement}. If $\QV\neq\Qcaut$, this algorithm is not guaranteed to find an optimal policy anymore, but is still guaranteed to act safely as long as the \Qcaut~estimation algorithm has converged to a stationary value.

\subsubsection{Safety Q-Learning}
The previous algorithm explores large portions of the state-action space that may not be useful for the reward-maximization task. The idea of Safety Q-Learning is to\emph{ guide} the exploration with this reward-maximization task while\emph{ constraining} it to state-actions that are thought to be viable. Compared to Safety-Q switch, the Safety Q-Learning algorithm does not have a phase during which it explores for the viable set. Instead, it directly follows the Q-Learning update, and keeps updating the safety measure along the way. By doing so, we hope that the safety measure will primarily expand in regions that are deemed interesting by Q-Learning. Of course, the samples are now going to be less informative for the safety measure\footnote{The algorithm in~\cite{heim2020learnable} uses an active sampling criterion to maximize the information gain}. Hence, we expect this algorithm to be better at quickly accumulating reward, while having a slower and less accurate estimation of the viable set, especially in regions that are not interesting for reward maximization.% The algorithm is summarized in Algorithm~\ref{alg:safety q learning}.
\paragraph{Learning what the agent needs to know}The Safety Q-Learning algorithm enjoys the same strong theoretical guarantees as Safety-Q switch if it manages to correctly learn the viable set~\QV. However, the goal of this algorithm is not to learn the whole viable set, but only to do it in specific, useful regions. In particular, \Qcaut~may converge to a superset of \QV, but only in regions where the value is not interesting. Wherever the optimal behaviour lies, \Qcaut~should locally match \QV~eventually.

\subsubsection{Soft Safety Q-Learning}
By following the Q-Learning update instead of actively sampling \LQhat, the Safety Q-Learning algorithm generates a lot of samples that are uninformative for the safety measure. Indeed, the agent may find itself repeatedly taking actions that have been known to be safe for a long time. In practice, this can give rise to problems by creating an unbalanced dataset\footnote{This can become a problem for example when the safety measure is approximated with a neural network instead of a GP} or, in the case of GPs, by flooding the GP's dataset with points adding little information\footnote{The time complexity of making a prediction with a GP increases polynomially with the number of samples~\cite{williams2006gaussian}.}. A way out of this problem is to introduce\emph{ sample selection}. \par
When are samples informative? Consider that we have evaluated the safety measure at $(s, a)\in\Qcaut$. Recall that we can compute from the output of the GP the probability that the measure at $(s, a)$ is positive. If this probability is high, the GP is confident that the considered state-action is viable. If it is lower, it does not mean that the GP considers this point to be unviable (since it is in \Qcaut), but rather that this state-action\emph{ should} be viable, but the confidence is low. Hence, refining the estimate of \LQhat~here would be useful. Consequently, we define a subset \Qsoft~of \Qcaut~which contains state-actions where the safety measure is above $\lambda_\text{caut}$ with probability at least $\gamma_\text{caut}$, but\emph{ at most} $\gamma_\text{soft}$, where $\gamma_\text{soft}\in[\gamma_\text{caut}, 1]$ is a new parameter. In practice, this corresponds to defining a third probabilistic level set, in addition to \Qcaut~and \Qopt. Then, the agent updates the safety measure with a new sample only if it was collected outside of \Qsoft.\par
The consequence of this is that the agent only updates the safety measure if the sampled state-action is near the current boundary of \Qcaut. Intuitively, this can be understood as a constraint activation condition: the estimate of \QV~is only expanded when the agent believes that the value of states that are currently outside of this estimate are interesting. 

\subsection{Metrics} \label{sec:metrics}
\paragraph{Choice of the metrics} The original goal is to derive algorithms that learn how to maximize the return while quickly learning how not to fail. Two important metrics are then the total expected return\footnote{We could have chosen the total discounted return instead, which is the return that the agents learn to maximize. This does not really make a difference, since the agents do not have any notion of time.} $G$ and the probability of failing $F$:
\begin{align}
	G~&=~\expected_\pi\left[\sum_{t=0}^\Tf R_{t+1}\right],\\
	F~&=~\P_\pi\left[\exists t, S_t\in\SF\right]~=~\expected_\pi\left[\sum_{t=0}^\Tf \indicator_\SF(S_{t+1})\right],
\end{align}
where $\pi$ is the evaluated policy\footnote{For the second expression of $F$, recall that failure is a terminal state: if the agent fails, only one of the terms in the sum is non-zero.}. For the agents that build an estimate of \QV, we also define the following two metrics:
\begin{align}
	K = \frac{\card{\QV\setminus\Qcaut}}{\card{\QV}},\\
	N = \frac{\card{\Qcaut\setminus\QV}}{\card{\QV}},
\end{align} 
where $\card{\cdot}$ is the cardinality of a set. We call $K$ the\emph{ conservativeness} of the agent: it is the proportion of the viable set that the agent considers unsafe. Similarly, the\emph{ negligence} $N$ is the normalized number of unviable state-actions that the agent considers to be safe.

\paragraph{Evaluating the metrics} We evaluate these four metrics regularly during training. To evaluate the conservativeness and the negligence, we simply brute-force the computation of the set \Qcaut~and compare it with the true viable set, that is computed beforehand using the methods from~\cite{heim2019beyond}. The failure rate and the expected reward are evaluated by Monte Carlo methods. The current policy of the agent is evaluated on a number of\emph{ measurement episodes} that are not used for training\footnote{Hence, the policy does not change when it is evaluated.}. The results are then averaged. The main bottleneck to precise and frequent evaluations is the available computational power: we chose to evaluate the policies every $10$ training episodes, and use $20$ measurement episodes each time. Finally, each agent was trained $10$ times on each environment, and the metrics were once again averaged across trainings. As a consequence, each measurement point is an average over $200$ episodes.

\subsection{Sample forgetting} \label{sec:sample forgetting}
Gaussian processes get increasingly expensive to evaluate as the size of their dataset grows: the time-complexity of this operation is $\mathcal{O}(n^3)$, where $n$ is the number of samples~\cite{williams2006gaussian}. For this reason, limiting the number of samples quickly becomes a necessity. Doing so, giving priority to new samples makes a lot of sense in our setting: the value with which the dataset is updated depends on the past predictions on the GP. Hence, early values are\emph{ very} sensitive to the initialization. Forgetting them is thus a way of limiting the number of points while decreasing the long-term sensibility to the initial conditions.\par
The solution we implemented is based on a nearest neighbours approach. As new samples are added, old samples that are closer than a user-defined\emph{ forgetting radius} are forgotten. This general rule has two exceptions for the GP modeling the safety measure. First, failures cannot be forgotten, since they are the only data points where we update with the ground truth instead of an estimate. Second, failures cannot\emph{ cause} other samples to be forgotten. This enables for example to learn sharp shapes for the viable set, as it is the case for the SLIP model (see Figure~\ref{fig:state action spaces:slip}). For both environments, we chose a forgetting radius of $0.05$, since it enabled a satisfactory learning while keeping the duration of each batch of $10$ trainings between $3$ and $7$ hours\footnote{Depending on the environment and on the agent.} on a desktop PC with processor AMD Phenom 2 X4 with 4GB of RAM.

\section{Results} \label{sec:results}
All of the previous algorithms were run on both environments with the parameter values presented in Table~\ref{tab:parameters}. The parameters related to the safety measure were tuned from the results of~\textcite{heim2020learnable}. The low value for the discount rate $\gamma$ was chosen to illustrate the effect of unviable state-action pairs on the hovership example, and was kept consistent across all experiments. As in~\cite{heim2020learnable}, the Gaussian processes used kernels of the Matérn family~\cite[Chapter\,18]{williams2006gaussian}, which have three parameters: a scalar amplitude, and one length scale for each input dimension. The length scales describe how far two samples should be so the GP considers their outputs to be independent. The GPs modeling the safety measure and the value function were parameterized identically, and their parameters were tuned by optimizing on the ground truth of the safety measure. The initialization of the estimates of the safety measure and the value function were kept consistent across all agents in a given environment. The values of the metrics were computed by the procedure described in Section~\ref{sec:metrics}

\begin{table}
	\centering
	\begin{tabular}{l|cc}
		Name & Symbol & Value\\
		\thickhline
		Exploration parameter & $\varepsilon$ & $0.1$\\
		Discount rate & $\gamma$ & $0.2$\\
		Step size & - & $0.6$\\
		Optimistic confidence & $\gamma_\text{opt}$ & $0.6\to0.8$\\
		Cautious confidence & $\gamma_\text{caut}$ & $0.7\to0.8$\\
		Cautious threshold & $\lambda_\text{caut}$ & $0$\\
		Soft constraint confidence & $\gamma_\text{soft}$ & $0.75\to0.85$\\
		Switch time & $\tswitch$ & $200$\\
		Number of episodes & - & $500$\\
		\thickhline
	\end{tabular}
	\caption{Values of the parameters. The first three parameters are the standard parameters of Q-Learning with an $\varepsilon$-greedy exploration policy. The confidence parameters are linearly increased during training. For the Safety-Q switch algorithm, the increase is until time $\tswitch$.}
	\label{tab:parameters}
\end{table}

\begin{table}
	\centering
	\begin{tabular}{l|cc}
			& Hovership & SLIP \\
		\thickhline
		Safety-Q Switch & 5.84 & 52.94\\
		Safety Q-Learning & 19.52 & 58.22\\
		Soft Safety Q-Learning & 22.84 & 68.84\\
		Q-Learning & 92.64 & 93.86\\
		Penalized Q-Learning & 76.62 & 72.18\\
		\thickhline
	\end{tabular}
	\caption{The percentage of episodes that ended in failure during training, averaged over 10 trainings. These values are very different from the ones in Figures~\ref{fig:results:hovership:expected failure} and~\ref{fig:results:slip:expected failure} for the Q-Learning variants because, here, the $\varepsilon$-greedy exploration policy is active.}
	\label{tab:failures exploration}
\end{table}
\subsection{Hovership}


\begin{figure}[t]
	\centering
	\begin{subfigure}{0.39\textwidth}
		\centering
		\includegraphics[width=\textwidth]{metrics/hovership_expected_reward}
		\caption{Expected reward $G$}
		\label{fig:results:hovership:expected reward}
	\end{subfigure}
	\begin{subfigure}{0.39\textwidth}
		\centering
		\includegraphics[width=\textwidth]{metrics/hovership_expected_failure}
		\caption{Expected failure $F$}
		\label{fig:results:hovership:expected failure}
	\end{subfigure}
	\begin{subfigure}{0.2\textwidth}
		\centering
		\includegraphics[width=\textwidth]{metrics/legend_hovership}
	\end{subfigure}
	\newline
	\begin{subfigure}{0.39\textwidth}
		\centering
		\includegraphics[width=\textwidth]{metrics/hovership_conservativeness}
		\caption{Conservativeness $K$}
		\label{fig:results:hovership:conservativeness}
	\end{subfigure}
	\begin{subfigure}{0.39\textwidth}
		\centering
		\includegraphics[width=\textwidth]{metrics/hovership_negligence}
		\caption{Negligence $N$}
		\label{fig:results:hovership:negligence}
	\end{subfigure}
	\begin{subfigure}{0.2\textwidth}
		\hfill
	\end{subfigure}
	\caption{Training curves of the agents on the hovership environment. The standard deviation is not plotted for clarity and because it does not add any major information. For penalized environments, Figure~\ref{fig:results:hovership:expected reward} corresponds to the reward without the penalty.}
	\label{fig:results:hovership}
\end{figure}

The safety measure is initialized conservatively on this environment: the initial estimate of $\Qcaut$ is mainly contained in $\QV$, as it can be seen by the low initial values of the negligence $N$ (Figure~\ref{fig:results:hovership:negligence}). The learning curves of all algorithms are presented on Figure~\ref{fig:results:hovership}, and the learned value maps and safe sets on Figure~\ref{fig:learned sets:hovership}. The general trends that emerge from these results is that all four safety-aware algorithms manage to achieve satisfactory return, but only the ones estimating the viable set learn safe behaviours: this comes from the existence of the unviable set, as we see it now.\par
The unpenalized Q-Learning algorithm does not learn how to stay safe. The low discount rate $\gamma$ prevents any long-term planning, which leads the agent to maximize the immediate reward and go down as fast as possible, thus failing in a few steps (Figure~\ref{fig:learned sets:hovership:q learning}). As a result, the unpenalized Q-Learning algorithm is very bad at maximizing the total reward. \par
This changes drastically when adding a high penalty. The high value of the penalty\enquote{ back-propagates} from the failure set and lowers unviable values, while being scaled by $\gamma$ at every step: the agent learns that it should avoid failing, at least in the short term. As a result, the agent learns how to act in a safer way, but still not safely (Figure~\ref{fig:results:hovership:expected failure}). Instead, it learns how to\emph{ fail slowly} (Figure~\ref{fig:learned sets:hovership:penalized}): once in the unviable set, it stays there for as long as possible and therefore collect high rewards (since the unviable set has better rewards than the viable one). This results in an unsafe policy that achieves better total reward in the given time horizon than the optimal safe one. As a matter of fact, the agent sometimes (about 25\% of the time) managed to stay in the unviable set long enough that the environment terminates before it fails. This almost surely explain how the penalized agent manages to collect higher rewards than the supposedly optimal safety-aware algorithms (Figure~\ref{fig:results:hovership:expected reward}). This shows that the agent is able to survive for a long time in the unviable set: therefore, even a high penalty value like $p=10000$ is unable to make the learned policy safe.\par
Safety measure-based algorithms do not suffer from such an issue, and all quickly achieve almost-safe behaviour (Figure~\ref{fig:results:hovership:expected failure}). Safety-Q Switch is the fastest algorithm to learn how to act safely, and quickly achieves almost-zero conservativeness and negligence(Figures~\ref{fig:results:hovership:conservativeness}--\ref{fig:results:hovership:negligence}). The steady non-zero value of the conservativeness is understood as a consequence of the forgetting radius discussed in Section~\ref{sec:sample forgetting}: the agent is unable to find the correct border between the viable and the unviable sets. Safety-Q Switch methodically explores the whole viable set during its exploration phase (Figure~\ref{fig:learned sets:hovership:switch}), and therefore acts suboptimally from the reward-maximization perspective during early steps. This changes when it switches behaviour at $t=\tswitch$: then, the algorithm starts exploiting all the knowledge it has collected about the reward function during the first phase, and instantaneously catches up with reward-driven algorithms.\par
The Safety Q-Learning and Soft Safety Q-Learning algorithms are therefore better at accumulating reward in early steps, but their advantage disappears after $\tswitch$. These algorithms also have a surprisingly high failure rate: we would expect them to be much closer to the one of Safety-Q Switch. A possible explanation is their aggressive exploration policy. Indeed, when not acting greedily, they uniformly sample another action inside \Qcaut. Therefore, they may end up in regions of the state-action space in which they do know yet what actions are viable or not, especially in the beginning. Finally, Safety Q-Learning and its Soft variant seem to neglect the upper-right region of the state-action space in the exploration. They indeed quickly realize that this region is not interesting for the reward, contrary to Safety-Q Switch.\par
Table~\ref{tab:failures exploration} shows that the safety measure-based algorithms need to sample failure far less often than Q-Learning variants. The fundamental reason is the $\varepsilon$-greedy exploration policy of Q-Learning: even if it has already discovered the true value map and the optimal policy, it regularly deviates from this optimal policy and takes a random action for exploration. Therefore, it very often samples a failure. This argument indicates that safety measure based methods are more sample-efficient when sampling failures.

\begin{figure}[t]
	\centering
	\begin{subfigure}{0.31\textwidth}
		\centering
		\includegraphics[width=\textwidth]{safe_sets/hovership_safety_q_switcher}
		\caption{Safety-Q switch}
		\label{fig:learned sets:hovership:switch}
	\end{subfigure}	
	\begin{subfigure}{0.31\textwidth}
		\centering
		\includegraphics[width=\textwidth]{safe_sets/hovership_safety_q_learning}
		\caption{Safety Q-Learning}
		\label{fig:learned sets:hovership:safety q learning}
	\end{subfigure}	
	\begin{subfigure}{0.31\textwidth}
		\centering
		\includegraphics[width=\textwidth]{safe_sets/hovership_soft_safety_q_learning}
		\caption{Soft Safety Q-Learning}
		\label{fig:learned sets:hovership:soft safety q learning}
	\end{subfigure}\newline
	\begin{subfigure}{0.39\textwidth}
		\includegraphics[width=\textwidth]{safe_sets/hovership_unpenalized}
		\caption{Q-Learning}
		\label{fig:learned sets:hovership:q learning}
	\end{subfigure}
	\begin{subfigure}{0.39\textwidth}
		\captionsetup{width=1.1\textwidth}
		\includegraphics[width=\textwidth]{safe_sets/hovership_penalized}
		\caption{Penalized Q-Learning ($p=10000$)}
		\label{fig:learned sets:hovership:penalized}
	\end{subfigure}
	\caption{The learned value maps and \Qcaut~set. The coloured dots are the state-actions visited during training. Blue samples are used by both GPs, whereas the red ones are only used by the GP modeling the value function. On Figures~\ref{fig:learned sets:hovership:switch}--\ref{fig:learned sets:hovership:soft safety q learning}, the sets \QV~and \Qcaut~are the solid black lines.}
	\label{fig:learned sets:hovership}
\end{figure}
\subsection{Spring-loaded inverted pendulum}

\begin{figure}[t]
	\centering
	\begin{subfigure}{0.39\textwidth}
		\centering
		\includegraphics[width=\textwidth]{metrics/slip_expected_reward}
		\caption{Expected reward $G$}
		\label{fig:results:slip:expected reward}
	\end{subfigure}
	\begin{subfigure}{0.39\textwidth}
		\centering
		\includegraphics[width=\textwidth]{metrics/slip_expected_failure}
		\caption{Expected failure $F$}
		\label{fig:results:slip:expected failure}
	\end{subfigure}
	\begin{subfigure}{0.2\textwidth}
		\centering
		\includegraphics[width=\textwidth]{metrics/legend_slip}
	\end{subfigure}
	\newline
	\begin{subfigure}{0.39\textwidth}
		\centering
		\includegraphics[width=\textwidth]{metrics/slip_conservativeness}
		\caption{Conservativeness $K$}
		\label{fig:results:slip:conservativeness}
	\end{subfigure}
	\begin{subfigure}{0.39\textwidth}
		\centering
		\includegraphics[width=\textwidth]{metrics/slip_negligence}
		\caption{Negligence $N$}
		\label{fig:results:slip:negligence}
	\end{subfigure}
	\begin{subfigure}{0.2\textwidth}
		\hfill
	\end{subfigure}
	\caption{Training curves of the agents on the SLIP environment. The standard deviation is not plotted for clarity and because it does not add any major information. For penalized environments, Figure~\ref{fig:results:slip:expected reward} corresponds to the reward without the penalty.}
	\label{fig:results:slip}
\end{figure}

The safety measure is initialized optimistically on this environment, and a lot of unsafe state-actions are considered safe initially: this is shown by the high initial negligence and low initial conservativeness (Figures~\ref{fig:results:slip:conservativeness}--\ref{fig:results:slip:negligence}). The learning curves of all algorithms are presented on Figure~\ref{fig:results:slip}, and the learned value maps and safe sets on Figure~\ref{fig:learned sets:slip}. On this task, the penalized method performs similarly to the ones based on the safety measure on the expected return, and achieves a much lower failure rate. \par
The low failure rate achieved by the penalized Q-Learning method was expected. Indeed, contrary to the hovership environment, the penalty here is not hidden by the small discount factor. Therefore, the agent rapidly learns that failing actions are not interesting. \par
More surprising is the poor performance of safety-based methods in terms of failure rate (Figure~\ref{fig:results:slip:expected failure}). The low negligence they all achieve indicates that they quickly learn a conservative estimate of the viable set (Figure~\ref{fig:results:slip:negligence}), and Figures~\ref{fig:learned sets:slip:switch}--~\ref{fig:learned sets:slip:soft safety q learning} show that this conservativeness does not prevent them from knowing how to behave in almost-every situation: except at the very top and bottom of the viable set, all algorithms know how to perform in a viable way. We do not have a definitive explanation for these poor performances. Our best hypothesis is that the hyperparameters of the Gaussian Process modeling the safety measure are incorrectly tuned for this problem. Indeed, GPs with Matérn kernels assume smooth variations of the data over the lengthscale of the kernel. The viable set of the SLIP environment is very narrow for low and high values of the state (Figure~\ref{fig:state action spaces:slip}): therefore, the Q-safety function varies sharply in these regions. Moreover, these regions play a critical role here since the reward incentivizes the agent to go as low as possible. We also believe that this is the reason why the conservativeness of some agents start rising again on Figure~\ref{fig:results:slip:conservativeness}. We plan on exploring this hypothesis in the coming weeks. It can still be noted that, even with this poor parameterization, Safety-Q Switch manages to mitigate its failure rate to reasonable levels.\par
Despite their general poor performance, Safety Q-Learning seems to achieve better return and a lower failure than its Soft counterpart. This indicates that the criterion we use to decide whether a sample is informative for the safety measure is a bit off, since it excludes samples that enable better performance. This can be seen on Figure~\ref{fig:learned sets:slip:soft safety q learning}: the safety measure indeed only uses a very small number of viable samples. The added samples are still very meaningful for the viable set: they typically correspond to regions where refining the initial estimate was indeed necessary. However, a relaxation of this criterion should be considered.\par
Finally, it should be noted that here again safety measure-aware methods largely outperform Q-Learning based methods in terms of the number of failures\emph{ during} training (Table~\ref{tab:failures exploration}). This is also caused by the exploration policy of Q-Learning methods, since they are likely to take a random action at every step. This is a clear advantage of methods estimating the safety constraints, since they avoid such a problem\emph{ by design}.

\begin{figure}[t]
	\centering
	\captionsetup[subfigure]{width=1.1\textwidth}
	\begin{subfigure}{0.31\textwidth}
		\centering
		\includegraphics[width=\textwidth]{safe_sets/slip_safety_q_switcher}
		\caption{Safety-Q switch}
		\label{fig:learned sets:slip:switch}
	\end{subfigure}	
	\begin{subfigure}{0.31\textwidth}
		\centering
		\includegraphics[width=\textwidth]{safe_sets/slip_safety_q_learning}
		\caption{Safety Q-Learning}
		\label{fig:learned sets:slip:safety q learning}
	\end{subfigure}	
	\begin{subfigure}{0.31\textwidth}
		\centering
		\includegraphics[width=\textwidth]{safe_sets/slip_soft_safety_q_learning}
		\caption{\mbox{Soft Safety Q-Learning}}
		\label{fig:learned sets:slip:soft safety q learning}
	\end{subfigure}	\newline
	\begin{subfigure}{0.39\textwidth}
		\includegraphics[width=\textwidth]{safe_sets/slip_unpenalized}
		\caption{Q-Learning}
		\label{fig:learned sets:slip:q learning}
	\end{subfigure}
	\begin{subfigure}{0.39\textwidth}
		\includegraphics[width=\textwidth]{safe_sets/slip_penalized}
		\caption{Penalized Q-Learning ($p=100$)}
		\label{fig:learned sets:slip:penalized}
	\end{subfigure}
	\caption{The learned \Qcaut~set for the three algorithms based on the safety measure on the SLIP environment. Blue samples are used by both GPs, whereas the red ones are only used by the GP modeling the value function. On Figures~\ref{fig:learned sets:slip:switch}--\ref{fig:learned sets:slip:soft safety q learning}, the sets \QV~and \Qcaut~are the solid black lines.}
	\label{fig:learned sets:slip}
\end{figure}
\subsection{Discussion}
In the last sections, we have presented and benchmarked four algorithms to learn safe policies, three of them being based on building an estimate of the viable set \QV~to put a constraint on the actions available to the exploration strategy.\par
The method based on first building an estimate of the safe set, and then optimizing for the expected return is the more careful: it achieves the lowest failure rate when correctly parameterized, and a satisfying one when its parameters don't enable learning \QV~correctly. While building the estimate of the viable set, it also accumulates knowledge on the reward function, and this knowledge can be immediately used when switching behaviours. Therefore, even though this method is suboptimal during early steps, it quickly catches up on reward-focused methods after $\tswitch$. This emphasizes the importance of this switch parameter. If it is too small, the estimate of the viable set is not good enough and the agent will not act safely. If it is too high, the agent will take a long time before taking reward-maximizing actions. Moreover, the systematic exploration of the whole viable set can become a problem in high dimensional systems (where learning the whole set is computationally intractable) or if data collection is costly. \par
The second and third algorithms get rid of this exploration phase, and immediately try to learn how to maximize the reward by guiding the exploration with constrained Q-Learning updates. This enables them to primarily refine their estimate of the viable set in regions that are interesting from a reward perspective. However, they seem to fail systematically more often than the previous method. This may be caused by the aggressive constrained $\varepsilon$-greedy exploration policy they implement from the very beginning, when the estimate of the viable set is very unprecise. More local exploration policies - such as adding a local noise around the optimal action - should reduce this tendency. Overall, the main advantage of these methods over the previous one is that they do not have an initial inefficient exploration phase, and therefore are easier to tune and should be better suited for high-dimensional systems.\par
In the presence of unviable states, these three algorithms outperform penalty-based methods in terms of failure rate. Indeed, the penalty scales exponentially with the number of steps that can be taken in the unviable set before failing. It therefore needs to be very high in order to learn safe policies, even in simple examples like the hovership. On the other hand, penalizing failures has proven to be very effective when the unviable set is empty, which confirms experimentally the statements of Corollary~\ref{clry:penalized solves constrained}.\par
One key advantage that methods estimating safety have over penalized methods is their sample efficiency with the number of failures. Indeed, they manage to quickly learn the constraints and naturally stop violating them\emph{ during learning}. Penalized methods do not enjoy such a property, since their exploration policy (like $\varepsilon$-greedy) is generally unaware of the safety constraints, and therefore repeatedly sample failures. This is a major practical drawback to implement them on safety-critical systems, where the number of failures should be reduced during learning.