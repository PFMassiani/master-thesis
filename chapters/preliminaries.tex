\chapter{Preliminaries} \label{chap:prelim}

	In this chapter, we introduce the concepts that this work builds upon. We start by presenting in Section~\ref{sec:preliminary RL} the well-established RL formalism and its connection with MDPs. We also discuss the main differences between MDPs and CMDPs, as they will happen to be of critical importance to understand the limitations of the penalized methods of Chapter~\ref{chap:safety from viability}. We then introduce viability theory in Section~\ref{sec:preliminary viability}, which is a very general framework typically used to describe dynamical systems that avoid failure without converging to an equilibrium. This powerful conceptual tool will enable us to formalize our definition of safety in Chapter~\ref{chap:safety from viability}, and will be central for the algorithms derived in Chapter~\ref{chap:benchmark}.
	
	\section{Reinforcement learning} \label{sec:preliminary RL}
		Herein, we give a brief introduction to RL. For a complete and thorough overview of the field, we refer the reader to the book~\cite{sutton2018reinforcement} by Sutton and Barto. The definitions and concepts presented here can be found in~\cite[Chapter\,3]{sutton2018reinforcement}.\par
		The general purpose of RL is learning what to do, that is, to map states to actions. The learner - also called the\emph{ agent} - is placed in an\emph{ environment}, and its goal is to maximize a\emph{ long-term reward}. The catch is that the agent is not told which actions are good: it has to discover this through\emph{ trial and error}.
		
		\subsection{Markov decision processes} \label{sec:preliminary mdps}
			The goal of RL is generally formalized through the mathematical model of MDPs, which are the simplest way to describe learning from interactions with an environment.
			\subsubsection{States, actions, and rewards} 
				The agent interacts with the environment in a sequence of discrete time steps $t = 0, 1, 2, 3, \hdots$. At each time step $t$, the agent is in a given state $S_t \in \S$ and can pick an action $A_t \in \A$ to transition to the next state, $S_{t+1}$. While this transition occurs, the agent also collects a scalar \emph{reward} $R_{t+1}$\footnote{In this work, we follow the convention adopted in~\cite{sutton2018reinforcement} and use $R_{t+1}$ instead of $R_t$ at time $t$ to emphasize the fact that the reward is determined\emph{ after} the action $A_t$ is chosen, jointly with $S_{t+1}$.}. Hence, a notion of\emph{ trajectory} naturally emerges from an agent evolving in an MDP:
				\begin{equation*}
					T = (S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, \hdots).
				\end{equation*}
				The set of all possible trajectories is noted $\T = (\S\times\A\times\R)^\Tf$, where \Tf~is the stopping time of the process and is defined below. What's more, $\Q = \S\times\A$ is called the\emph{ state-action set}, or state-action space\footnote{In all of this work, the words \enquote{set} and \enquote{space} will be used interchangeably when refering to the sets \S, \A, or \Q.}.
			\subsubsection{Environment and Markov property} 
				In this work, we will restrict ourselves to\emph{ finite} MDPs, that is where \S~and \A~are finite sets. Hence, the random variables describing the successive states, actions and rewards follow well-defined discrete probability distributions. A fundamental assumption in MDPs is that the current state of the agent contains all of the available information about the future: the\emph{ history} of the agent - that is, its trajectory up to the current time $t$ - dos not give any additional information. This is formalized in the following assumption:
				\begin{assumption}
					States and rewards satisfy the\emph{ Markov property}, that is, for all states $(s_0, s_1, \hdots, s_t)$ and actions  $(a_0, a_1, \hdots, a_t)$, and all $s'\in\S$~and $r\in\R$:
					\begin{align}
						\P(S_{t+1}=s'|S_0=s_0,A_0=a_0,\hdots,S_t=s_t,A_t=a_t)& = \P(S_{t+1}=s'|S_t=s_t,A_t=a_t) \label{eq:markov assumption state}\\
						\P(R_{t+1} = r|S_0=s_0,A_0=a_0,\hdots,S_t=s_t,A_t=a_t)& = \P(R_{t+1} = r|S_t=s_t,A_t=a_t) \label{eq:markov assumption reward}
					\end{align}
				\end{assumption}
				This fundamental assumption of MDPs states that the\emph{ transition probabilities} to the next state only depend on the current state and the chosen action, and not on the rest of the history of the agent. The function giving the probabilities of the next state $s'$ and the collected reward $r$ given the current state $s$ and action $a$ is called the\emph{ dynamics} of the environment:
				\begin{equation}
					f(s', r | s, a) = \P(S_{t+1} = s', R_{t+1} = r~|~S_t=s, A_t=a). \label{eq:dynamics}
				\end{equation}
				If the environment is\emph{ stationary} - which we will assume from now on - the dynamics does not depend on time.\par
				The Markov assumption should be understood as a condition on the state rather than on the environment. The state must indeed include all information that make a difference on the future of the agent.
			\subsubsection{Return function}
				When interacting with its environment, the agent's goal is to maximize a \enquote{long term reward}, also called the\emph{ return}\footnote{This should not be mixed up with the reward: the reward is an instantaneous signal that the agent collects, and the return is the way these instantaneous reward are aggregated.}. Surely, this return should be related to the immediate reward: if the agent has collected the rewards $R_t, R_{t+1}, \hdots$ after the time step $t$, what is the associated return? To answer this question, we must define an aggregation function that will be the agent's definition of the return.
				\paragraph{Episodic tasks} There are several reasonable definitions for the return. For example, the\emph{ total return} is simply the sum of all collected rewards:
				\begin{equation}
					G_t = R_{t+1} + R_{t+2}  + \cdots + R_\Tf = \sum_{u=t+1}^\Tf R_u, \label{eq:total return}
				\end{equation}
				where $\Tf$ is the final time step\footnote{In general, \Tf~is also a random variable since the stopping time is not known in advance. In this work, we will not consider such a technicality and will assume \Tf~to be constant, although our results can extend to the case where \Tf~is random.}. This definition is particularly suited when there is such a notion of final time step. In that case, the interactions between the agent and the environment naturally break into subsequences called\emph{ episodes}, and the MDP is called episodic. Such a setting describes, for example, an agent whose goal is to find a flag in an environment or an agent playing a video game. The episode is over when the agent reaches a\emph{ terminal state}: the flag is found, the game is over. After an episode ends, the agent is generally reset in the environment and a new episode can start, independently of the outcome of the previous one. 
				
				\paragraph{Continuing tasks} When the agent-environment interaction does not break into such episodes, the total return is in general not well-suited since it can easily take infinite values. In that case, the task is said to be\emph{ continuing}. A simple way-out of these infinite returns is to consider the\emph{ discounted return}\footnote{There are other ways of defining finite returns for continuing tasks (such as the average return), but we will not consider them here.}, where the agent discounts the rewards in the future and maximizes the sum of these discounted rewards:
				\begin{equation}
					G_t = \sum_{u=0}^\infty \gamma^u R_{t + u + 1}, \label{eq:discounted return}
				\end{equation}
				where $0 \leq \gamma < 1$ is the\emph{ discount rate}, or\emph{ discount factor}. The value of the discount factor defines how much the agent values long-term rewards compared to immediate ones. If $\gamma = 0$, the agent is myopic and will only care about its next immediate reward. Such an agent has no ability for long-term planning. Contrarily, as $\gamma$~gets close to 1, future rewards are more and more taken into account. The discounted reward model is also popular in other fields such as economy, where it is commonly used to describe interest rates. The fact that it naturally gives less importance to long-term reward can either be seen as a feature - since long-term rewards are generally subject to more uncertainty - or as a shortcoming, as the evaluation it gives of situations often leads to short-term decision making. Whether the discounted reward is adapted to describing the task at hand is another modeling question. Alternatives to this definition exist, but discussing them is beyond the scope of this work.\par
				We define a common notation in order to treat both settings in the same way. The total return of Equation~\eqref{eq:total return} can also be written as:
			\begin{equation}
				G_t = \sum_{u=0}^\Tf \gamma^u R_{t + u + 1}, \label{eq:unified return}
			\end{equation}
			with $\gamma = 1$. Similarly, the discounted return of~\eqref{eq:discounted return} can be written as in Equation~\eqref{eq:unified return} with $\gamma < 1$ and $\Tf = \infty$. Hence, we will take Equation~\eqref{eq:unified return} as the general definition of the return, and allow $\Tf = \infty$ whenever $\gamma < 1$.
		\subsubsection{Initial state} 
			The agent needs to be\emph{ initialized} at the beginning of the experiment (or at the beginning of each episode for episodic tasks). The law of the variable $S_0$ describing the initial state of the agent is a free parameter\footnote{In practice, this parameter is not free but is determined by the true system that we model. For example, the initial state is generally fully determined in a video game.} in MDPs. We call $\mu$ the probability distribution of the initial state:
			\begin{equation*}
				\forall s\in\S,~\P(S_0 = s) = \mu(s).
			\end{equation*}
			The support of $\mu$ is the set of states in which the agent can be initialized with a non-zero probability, and is noted:
			\begin{equation*}
				\support(\mu) = \{s\in\S~|~\mu(s) > 0\}.
			\end{equation*}
			So far, we do not put any assumption on $\mu$. Such assumptions will become in Chapter~\ref{chap:safety from viability} in order to ensure the existence of safe policies.
		\subsubsection{Policies and optimization objective} 
			At each time step $t$, the agent needs to choose an action. The decision rule that the agent follows is called a\emph{ policy}: when in state $s$, the agent will take the action $a$ with probability $\pi(a|s)\in[0,1]$. For a given state, a policy determines a probability distribution over the set of actions, and so the following identity holds:
			\begin{equation*}
			\sum_{a\in\A}\pi(a|s) = 1.
			\end{equation*} 
			Formally, a policy is just an ordinary function $\pi:\,\Q\to[0,1]$, and the space of policies is denoted $\Pi$.\par
			The attentive reader may notice that our definition of policies is quite restrictive. Indeed, nothing suggests in what we have already established that the decision rule of the agent should not depend on time, or should only use the information of the\emph{ current} state to decide what action to take (compared to using all the available history). Such policies are called\emph{ stationary Markov policies}, and~\cite[Theorem\,2.1]{altman1999constrained} states that the optimization problem defined below can be restricted to such policies without loss of generality. In the following, we will only consider stationary Markov policies and refer to them simply as \enquote{policies}.\par
			The choice of a policy, combined with the dynamics and the distribution $\mu$ of the initial state, induces a probability distribution over the set of trajectories. Because of the stochasticity of both the dynamics and the policy, the return itself is a random variable defined on that set of trajectories. Hence, the agent's goal is to maximize the expected value of the return:
			\begin{equation}
					\underset{\pi\in\Pi}{\text{maximize}}\quad\expected_\pi\left(G_t\right). \label{eq:mdp problem}
			\end{equation}
			This is the fundamental optimization objective of MDPs that RL algorithms aim at solving. The main difference with classical optimal control is that, in RL, the objective function cannot be easily evaluated.
		\subsubsection{Value functions and Bellman equations} 
			A useful tool for solving Problem~\eqref{eq:mdp problem} is\emph{ value functions}\footnote{Value functions are in fact so convenient that they are often used to define the RL optimization problem~\eqref{eq:mdp problem}~\cite{sutton2018reinforcement}. We chose not to take this approach because value functions do not easily extend to CMDPs.}. The key idea here is to define what the\emph{ value of a state} (or a state-action) is under a given policy:
			\begin{definition}
				Let $\pi\in\Pi$. The state and state-action value functions of $\pi$ are:
				\begin{alignat}{7}
				& V_\pi\,&:&\, &s&\in\S & & \mapsto \expected_\pi\left[G_t~\middle|~S_t = s\right]. \label{eq:state value} \\
				& Q_\pi\,&:&\, (&s&, a)\in\Q & & \mapsto \expected_\pi\left[G_t~\middle|~S_t = s, A_t = a\right]. \label{eq:stateaction value}
				\end{alignat}
			\end{definition}
			According to the stationarity assumption, these functions do not depend on the time. The state (resp. sate-action) value function represents the expected return that the agent will get by following the policy $\pi$ when in state $s$ (resp. when in state $s$ and after taking action $a$). The state value function satisfies a recursive identity called the\emph{ Bellman equation}\footnote{The state-action value function also satisfies a Bellman equation.}~\cite[Section\,3.5]{sutton2018reinforcement}:
			\begin{equation}
				V_\pi(s) = \sum_{a} \pi(a|s) \sum_{s', r}f(s', r |s, a)\left[r + \gamma V_\pi(s')\right],\quad \forall s\in\S. \label{eq:bellman}
			\end{equation}
			The Bellman equation expresses a relation between the value of a state and the value of its successor states. We can think of what it means as a one-step-ahead lookup on the return the agent can collect. In a state $s$, the agent can take an action $a$ and reach a state $s'$ with reward $r$. The additional return that can be collected from $s'$ is $V_\pi(s')$: it aggregates all the possible rewards the agent will get when starting again from $s'$. The Bellman equation discounts this future return that the agent will only enjoy from the next step, and averages all these possible outcomes with their probability of happening: this defines the value of the current state $s$.
			\paragraph{Optimal value functions} The power of using value functions becomes clear when defining the\emph{ optimal value function}:
			\begin{definition}
				The optimal value functions of the MDP are:
				\begin{alignat}{6}
					V^*\,&:&\,&s&\in\S&\mapsto& & \max_{\pi\in\Pi}~V_\pi(s) \label{eq:optimal state value}\\
					Q^*\,&:&\,(&s&, a)\in\Q&\mapsto& & \max_{\pi\in\Pi}~Q_\pi(s, a) \label{eq:optimal state action value}
				\end{alignat}
			\end{definition}
			Now, the functions $V^*$~and $Q^*$ are the value functions associated to a policy, $\pi^*$. Namely, $\pi^*$ is the policy that always picks the action with the best value\footnote{We assume here that there is only one, but if this is not the case, any such action works.} for $Q^*$:
			\begin{equation}
				\pi^*(a|s) = \begin{cases}
				1,&\text{ if } a = \argmax_{a'}\,Q^*(s, a'),\\
				0,&\text{ otherwise}.
				\end{cases} \label{eq:optimal policy}
			\end{equation} The fact that this policy has $V^*$ and $Q^*$ as value functions is proven in~\cite[Chapter\,4]{sutton2018reinforcement}. And it is now straightforward to see that $\pi^*$ is an optimal policy for the optimization problem of MDPs~\eqref{eq:mdp problem}:
			\begin{equation*}
				\expected_{\pi^*}\left(G_t\right) \leq \max_\pi~\expected_\pi\left(G_t\right) = \sum_s \mu(s) V_\pi(s) \leq \sum_s \mu(s) V_{\pi^*}(s) = \expected_{\pi^*}\left(G_t\right).
			\end{equation*}
			So, the complicated optimization problem~\eqref{eq:mdp problem} can be solved by approximating the state-action value function $Q^*$, since an optimal policy can simply be derived from it. Most of the famous algorithms for RL like Q-Learning~\cite{watkins1992q}, TD methods~\cite{sutton2018reinforcement}, or SARSA~\cite{sutton2018reinforcement}, are methods that try to estimate this $Q^*$ function.
			
			\paragraph{Optimal policies} Markov decision processes are usually said to have a unique optimal policy~\cite{sutton2018reinforcement}, which then has to be the greedy policy $\pi^*$ of the previous paragraph. This property is only true under some assumptions on the initial state distribution $\mu$. For example, this property holds if the agent may be initialized in any state. However, consider the example where a region of the state space cannot be reached because the dynamics forbid it: the behaviour of the policy in that region does not matter, and the MDP has therefore several different optimal policies. This subtlety is particularly important when dealing with safe MDPs, where the goal is that the agent does not reach specific regions of the state space.
	
		\subsection{Constrained Markov decision processes} \label{sec:cmdps preliminaries}
		While Markov decision processes are commonly used for RL tasks, they are not always the best-suited model. We have seen in the introduction that CMDPs are particularly adapted to describe RL tasks with safety concerns. We provide here a brief introduction to CMDPs. This section is mainly based on \cite[Chapter\,3]{altman1999constrained}. We will not be as complete in our presentation of CMDPs as we have been for MDPs, and we refer the reader to~\cite{altman1999constrained} for a more detailed presentation.
		\subsubsection{A constrained optimization problem}
		The mathematical formalism of CMDPs is almost identical to the one of MDPs. In this work, we only consider MDPs constrained by a single scalar cost. A CMDP is therefore defined by its state space \S, action space \A, dynamics $f$, reward $R$, initial state distribution $\mu$, and cost function $c: \Q\to\R$. Similarly to the reward, the cost is a scalar signal that the agent samples at every step. The agent aggregates this cost with the cost discount factor $\gammabar$. Note that the cost function is different from the reward function: the latter defines the maximization objective, whereas the former defines the constraint.\par
		We say that an agent is solving the CMDP when it is solving the following optimization problem:
		\begin{equation}
		\begin{aligned}
			\underset{\pi}{\text{maximize}} & \quad\expected_\pi\left[G_t\right], \\
			\text{s. t.} & \quad\expected_\pi\left[\sum_{t=0}^\Tf\gammabar^tc(S_t, A_t)\right] \leq d,
		\end{aligned} \label{eq:cmdp problem}
		\end{equation}
		where $d\in\R$ is the maximal aggregated cost. Note that Problem~\eqref{eq:cmdp problem} is very similar to Problem~\eqref{eq:mdp problem}, with the difference that the only policies allowed are the ones that collect less cost than $d$ for the initial state distribution $\mu$.
		
		\begin{example}[Chance constraints are a special case of cost-constrained CMDPs] \label{ex:chance constraints}
			For example, assume we are given a safe set~$\S_\text{safe}\subset\S$, and a maximal acceptable risk $\delta$. Also assume that states outside of $\S_\text{safe}$ are terminal. We want to constrain the policies to stay in $\S_\text{safe}$ with probability at least $1 - \delta$:
			\begin{equation*}
				\P_\pi\left[\bigcap_{t=0}^\Tf\{S_t\in\S_\text{safe}\}\right] \geq 1 - \delta.
			\end{equation*}
			By taking $\gammabar = 1$ and $c(s, a) = \indicator_{\S\setminus\S_\text{safe}}(s)$, we have:
			\begin{equation*}
			\expected_\pi\left[\sum_{t=0}^\Tf\gammabar^tc(S_t, A_t)\right] = 1 - \P_\pi\left[\bigcap_{t=0}^\Tf\{S_t\in\S_\text{safe}\}\right].
			\end{equation*}
			Hence, choosing $d = \delta$ gives the desired constraint.
		\end{example} 
		\subsubsection{Optimal policies}
		One of the main differences between standard MDPs and CMDPs is their set of optimal policies. We have seen in Section~\ref{sec:preliminary mdps} that there always exists a deterministic policy that solves an MDP. This came from the strong relationship between optimal policies and the optimal value function. This property is not preserved in CMDP. We give a simple example to understand why below, and refer the reader to~\cite{altman1999constrained} for a more detailed explanation.\par
		The main theoretical tool used to study CMDPs is the\emph{ occupation measure} of a policy $\pi$:
		\begin{equation}
			\eta_\pi\,:\,(s, a)\in\Q\mapsto \lim_{t\to\Tf}\expected\left[\frac{1}{t}\sum_{u=0}^t\indicator_{\{s, a\}}(S_t, A_t)\right].
		\end{equation}
		The occupation measure intuitively represents the fraction of time spent in state-action $(s, a)$. A policy $\pi$ can be easily retrieved from its occupation measure $\eta$ by noting that:
		\begin{equation}
			\pi(a|s) = \frac{\eta(s, a)}{\sum_{a'\in\A}\eta(s, a')}.
		\end{equation}
		Hence, a trick similar to what we did for value functions can be used here as well: solve for the optimal occupation measure directly, instead of finding the policy from the complicated problem~\eqref{eq:cmdp problem}. And it turns out that this is much easier a problem, since it boils down to solving a constrained linear program~\cite[Theorem\,3.3]{altman1999constrained}, for which lots of methods exist.
		
		\paragraph{Intuitive explanation of stochastic optimal policies} Why is there such a difference between MDPs and CMDPs? Why don't CMDPs have deterministic optimal policies in general? Let us provide some intuition on this question based on the example of Figure~\ref{fig:cmdps stochastic optimal}. An unconstrained agent - that is, one which does not care about the cost - simply goes to $s=2$ and stays there, collecting a reward of $10$ every time. Now, consider a constrained agent with $\gammabar = 0.9$ and $\delta = 1$. It has an upper bound on the cost it is allowed to collect. If the agent goes in $s=2$ and stays there, as the unconstrained agent does, its expected discounted cost is $\frac{1}{1 - 0.9} = 10 > \delta$: this solution is not feasible. The other deterministic option is to go in $s=1$ and stay there: in this case, the expected cost is simply $0$. This is feasible, but suboptimal. Indeed, the agent could collect more reward by visiting $s=2$ from time to time, and the frequency of its visit there is bounded by the total expected cost the agent is allowed to collect. Therefore, the optimal constrained agent strikes a balance between the reward-optimal policy of the unconstrained agent and other actions that are less costly and permit an acceptable average cost. On this simple example, we can already infer that this stochastic behaviour should stop when the agent is not allowed to collect\emph{ any} cost, that is, when $\delta = 0$. We will see in Chapter~\ref{chap:safety from viability} that this is indeed the case.
		\begin{figure}
			\centering
			\includegraphics{cost_constraint_example}
			\caption[Constrained MDP example]{Here, $r$ is the reward collected when entering the state, and $c$ is the cost. The unconstrained optimal policy goes and stays in $s=2$. The constrained one switches between $s=1$ and $s=2$ depending on the accepted cost.}
			\label{fig:cmdps stochastic optimal}
		\end{figure} 
		
		\paragraph{Solving CMDPs} Since CMDPs can be expressed as a linear program, they can be solved using tools from linear programming. The main advantage of this is that linear programs can be very efficiently solved. For example, they always satisfy\emph{ strong duality}, which allows them to be treated as an unconstrained optimization problem simply by penalizing points that do not satisfy the constraints. Algorithms based on this linear program suffer from two main issues. The first one is scalability: the number of variables in the linear program scales linearly with the number of state-action pairs, and so exponentially with the number of dimensions of the problem. The second problem is that this approach is fundamentally model-based, since the linear program itself depends on the model. Hence, it greatly limits the applications of this result. Recently, Paternain et al.~\cite{paternain2019safe} have re-discovered the strong duality result for chance constraints in safe sets, and derive a primal-dual algorithm solving CMDPs in a model-free setting. We believe this approach is very promising: our main theoretical result (Theorem~\ref{thm:strong duality}) is an extension of their results, and was derived independently. Based on another approach, the CPO algorithm~\cite{achiam2017constrained} extends policy gradient methods - which are a local optimization scheme for approximately solving the MDP problem~\eqref{eq:mdp problem} - to propose the first scalable CMDP solver. This is a milestone in making CMDPs computationally tractable, but this algorithm is hard to generalize to methods other than policy gradients because it provides little understanding of the structure of the underlying problem (contrary to value-based or duality-based methods).
		
	\section{Viability theory for RL} \label{sec:preliminary viability}
	The definition of safety that we take in Chapter~\ref{chap:safety from viability} is based on a failure set that the agent should avoid at all times. Forbidding the agent to take steps that immediately result in failure is not sufficient, since it may be in a state where it is doomed to fail without having failed yet. An intuitive example would be a bipedal robot in the process of falling an without any means of controlling its fall, but before it touches the ground: it is already too late. This is a general question not only in RL, but in the field of dynamical systems. An answer to it was formalized by Aubin et al. in~\cite{aubin2011viability} and is called\emph{ viability theory}. Recently~\cite{heim2020learnable}, viability theory has started to be applied to RL problems. We provide here a brief introduction to viability theory in the context of MDPs.
	\subsection{Viability kernel, viable set}
	\subsubsection{Problem setting}
	In this section, and in the rest of this work, we restrict ourselves to\emph{ deterministic} MDPs. That is, the next state is a deterministic function of the previous state and action. With an abuse of notation (from the definition of the dynamics~\eqref{eq:dynamics}), we note:
	\begin{equation}
		s' = f(s, a),
	\end{equation}
	where $s'\in\S$ is the successor state of $s\in\S$ when taking action $a\in\A$. We are given a\emph{ failure set} $\SF\subset\S$ assumed to be\emph{ absorbing}: all states in the failure set are terminal and the agent needs to be reset after visiting them\footnote{This work can be extended to the case where this assumption is not true and agents can recover from failure, but this will ease the notations and the reasoning.}. This failure set can be defined arbitrarily, and typically contains bad outcomes the agent should avoid.
	\subsubsection{A toy model: the hovership}\label{sec:hovership}
	We use the toy model developed in~\cite{heim2020learnable} to illustrate the concepts. The toy model is described by the transition map of Figure~\ref{fig:hovership}. Intuitively, it  can represent a hovering spaceship affected by gravity, which is stronger near the ground. The ship can apply two level of thrusters, or allow itself to fall. Gravity gets stronger than the thrusters close to the ground. The failure set is $\SF = \{5\}$, and corresponds to crashing the ship.
	\begin{figure}
		\centering
		\includegraphics[width=0.6\textwidth]{hovership_viability.png}
		\caption[Hovership toy model]{The state space and viability kernel \SV~of the toy model (left), its transition map (middle), and viable and critical sets \QV~and \QC~(right). On the transition graph, the figures represent the next state when taking the column's action when in the row's state. Both \QV~and \SV~are highlighted in green. State-action pairs that result directly in failure are highlighted in orange, and the ones that result in unviable states are in yellow. The critical set \QC~is striped in light red. \figfrom{Adapted from \cite{heim2020learnable}}}
		\label{fig:hovership}
	\end{figure}
	\subsubsection{Viability kernel and viable set}
	We can now define the main mathematical object of viability theory~\cite[Chapter~1]{aubin2011viability}:
	\begin{definition}[Viability kernel]
		The \emph{viability kernel} $\SV\subset\S\setminus\SF$ is the maximal set of all states $s\in\S$, from which there exists an action that keeps the system inside $\SV$.
	\end{definition}
	By definition, states outside \SV~have already failed or will fail within finite time. On the hovership example, the viability kernel is $\SV = \{1, 2, 3\}$. From each of these states, it is possible to avoid failure at all times. Note that the viability kernel is in general not ergodic: it is impossible to go back to states $1$ or $2$ when in state $3$. The viability kernel is a superset of the ergodic set, which is used in~\cite{moldovan2012safe} and~\cite{turchetta2016safe}. In fact, the viability kernel is rigorously defined as the largest\emph{ controllable invariant}\footnote{A set is controllable invariant if there exists a policy that makes the agent stay in it at all times.} set of states with an empty intersection with the failure set~\cite{aubin2011viability}.\par
	Following the work of~\cite{heim2020learnable}, we define the viable set:
	\begin{definition}[Viable set]
		The \emph{viable set} $\QV\subset\Q$ is the maximal set of all state-actions $(s, a)$, such that $s' = f(s, a) \in\SV$.
	\end{definition}
	The viable set naturally extends the viability kernel in state-action space. There exists an intricate relationship between \SV~and \QV. For example, the viability kernel is empty if, and only of, the viable set is empty. Or similarly: for every state $s \in \SV$, there exists an action $a\in\A$ such that $(s, a) \in \QV$. These simple properties are immediate consequences of the definition, but will play a crucial role. On the hovership example, the viable set is the set of state-actions that end up in states $1, 2$ or $3$.\par
	Finally, we define a new notion called the critical set:
	\begin{definition}
		The\emph{ critical set} $\QC\subset\Q\setminus\QV$~is the set of state-actions $(s, a)\in\Q\setminus\QV$ such that $s\in\SV$.
	\end{definition}
	The critical set is the set of state-actions leaving the viability kernel. If the agent visits a state-action in the critical set, it is doomed to fail. Conversely, an agent in the viability kernel has to pass through the critical set in order to go to failure.\par
	We also for convenience the unviable set $\SU = \S\setminus\SV$. It is the set of states from which the agent fails within finite time.
	\subsection{A learnable safety measure} \label{sec:safety measure}
	We introduce here the notion of \Q-safety measure first proposed in~\cite{heim2020learnable}. This concept will be of particular interest to derive our algorithms in Chapter~\ref{chap:benchmark}.\par
	The safety measure is defined on state space as follows:
	\begin{definition}[Safety measure]
		The\emph{ safety measure} $\Lambda$ is the $n$-dimensional volume of the viable set \QV. When applied to a point $s\in\S$, $\Lambda(s) \in \R_{\geq0}$ is the measure of the corresponding slice of \QV.
	\end{definition}
	This definition is a formalization of the first properties of the viable set that we have highlighted in the previous paragraph: large values of $\Lambda(s)$ mean that there are a lot of actions that keep the agent in the viability kernel. Similarly, saying that $\Lambda(s) = 0$ exactly means that $s$ is not viable\footnote{In the finite setting. Indeed, there may exist unmeasurable sets of unviable actions in the infinite setting, but we will stay in the finite setting here.}. We immediately extend this notion to state-action space by mapping it through the dynamics:
	\begin{definition}[\Q-safety measure]
		The\emph{ \Q-safety measure} $\Lambda_\Q$ is defined as:
		\begin{equation*}
			\Lambda_\Q(s, a) = \Lambda[f(s, a)],\quad \forall (s, a) \in \Q. \label{eq:q safety measure}
		\end{equation*}
	\end{definition}
	The \Q-safety measure can be seen as a one-step look-ahead of the safety measure. It answers the question: \enquote{what is the measure of the state I end up in by taking this action ?}. The \Q-safety measure already takes the dynamics into account. An agent that knows this measure is able to always avoid failure (although this agent may be very bad at maximizing the return), by only following actions with a non-zero measure.
	\subsubsection{Learning the \Q-safety measure} The \Q-safety measure is generally unknown, even in the model-based setting. Indeed, computing it is equivalent to computing the viability kernel which is a task known to be computationally expensive~\cite{aubin2011viability}. \textcite{heim2020learnable} propose an algorithm to learn the \Q-safety measure from data by modeling it with a Gaussian process (GP). This algorithm will be central for the ones we present in Chapter~\ref{chap:benchmark}, so we briefly describe it here.
	\paragraph{Gaussian processes as function approximators} Gaussian processes are a powerful and adaptable machine learning tool commonly used to model unknown functions from data. More specifically, the prediction of a GP is composed of a\emph{ mean} - which is often taken as the prediction - and a\emph{ covariance} - which expresses how confident the GP is about the value it gives. For a more thorough explanation of GPs, we refer the reader to~\cite{williams2006gaussian}. 
	
	\paragraph{Probabilistic level sets} Given an approximation \LQhat~of the safety measure, the authors of~\cite{heim2020learnable} derive two estimates of the viable set: the\emph{ optimistic set}~\Qopt, and the\emph{ cautious set}~\Qcaut. They are parameterized by the three scalars $\gamma_\text{opt}, \gamma_\text{caut}$, and $\lambda_\text{caut}$, that can be interpreted as follows: for any state-action $(s,a)$ in \Qopt~(resp. \Qcaut), the safety measure is greater than $0$ (resp. $\lambda_\text{caut}$) at $(s, a)$ with probability $\gamma_\text{opt}$ (resp. $\gamma_\text{caut}$).  Hence, these two sets can be interpreted as different estimates of the viable set \QV~corresponding to difference confidence intervals. The parameters are chosen such that $\Qcaut \subset \Qopt$.
	
	\paragraph{Iterative estimation of the measure} The safety measure is estimated iteratively as follows~\cite{heim2020learnable}:
	\begin{itemize}
		\item Input: initial state $s_0$, initial safety measure $\LQhat$;
		\item Repeat until convergence, with $i$ the number of steps:
		\begin{enumerate}
			\item Evaluate the probabilistic level sets \Qopt~and \Qcaut from \LQhat;
			\item Choose an action $a$ to take:
			\begin{itemize}
				\item If an action is available in \Qcaut~from the current state, then pick such an action that maximizes the information gain\footnote{The covariance of the GP};
				\item Otherwise, pick the action that maximizes the probability of that action being in \Qcaut;
			\end{itemize}
			\item Take that action: end up in state $s_{i+1}$;
			\item If the agent failed: add $((s_i, a_i), 0)$ to the dataset of \LQhat, and reset the agent;
			\item Otherwise, compute the state measure estimate $\hat{\Lambda}(s_{i+1})$ from \Qopt, and add $((s_i, a_i), \hat{\Lambda}(s_{i+1}))$ to the dataset of \LQhat.
		\end{enumerate}
	\end{itemize}
	This algorithm has been able to correctly learn the viable sets of the examples of~\cite{heim2020learnable}.